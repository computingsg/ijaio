[
  {
    "id": 1,
    "question": "A drone flies 3km East and 4km North. This can be represented as vector [3, 4]. What is the straight-line distance from the starting point to the drone? (Hint: Think of a right-angled triangle).",
    "options": [
      "12km",
      "5km",
      "25km",
      "7km"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Using the Pythagorean theorem (a² + b² = c²), we get 3² + 4² = 9 + 16 = 25. Taking the square root gives √25 = 5 km. The drone's East and North distances form the two shorter sides of a right triangle, and the straight-line distance is the hypotenuse. Incorrect options come from common arithmetic mistakes — multiplying the sides, adding them directly, or forgetting the square root step.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Not quite! 12 km comes from multiplying 3 × 4, but that gives you the area calculation, not the distance. For a right triangle, you need the Pythagorean theorem: a² + b² = c². Try squaring each side (3² = 9, 4² = 16), adding them (25), then taking the square root (√25 = 5 km).",
      "1": "Excellent! Using the Pythagorean theorem: 3² + 4² = 9 + 16 = 25, and √25 = 5 km. The East and North distances form the two legs of a right triangle, and the straight-line distance (hypotenuse) is 5 km. This is a classic 3-4-5 right triangle — one of the most famous Pythagorean triples!",
      "2": "Almost there! You correctly computed 3² + 4² = 25, but forgot the final step — taking the square root. The Pythagorean theorem says a² + b² = c², so c² = 25 means c = √25 = 5 km. Always remember: you need the square root to get the actual distance!",
      "3": "Not quite! 7 km comes from simply adding 3 + 4, but that gives the total distance if the drone walked along the edges (like walking along two sides of a rectangle). The straight-line distance cuts diagonally and is always shorter. Use the Pythagorean theorem: √(3² + 4²) = √25 = 5 km."
    }
  },
  {
    "id": 2,
    "question": "In AI, we often describe an object using many numbers (features). If a house is described by [Price, Size, #Rooms, Age], we say it is a 4-dimensional vector. If we add 'Postal Code' to the description, what happens?",
    "options": [
      "It becomes a matrix",
      "The vector becomes longer in length but same dimension",
      "It stays 4-dimensional",
      "It becomes a 5-dimensional vector"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Each feature (piece of information) in a vector adds one dimension. Starting with 4 features [Price, Size, #Rooms, Age] gives a 4-dimensional vector. Adding 'Postal Code' as a 5th feature makes it a 5-dimensional vector. In AI, high-dimensional vectors are very common — some models work with hundreds or thousands of dimensions!",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Not quite! A matrix is a grid (table) of numbers with rows and columns — it's a different structure from a vector. Adding one more feature to a vector simply extends it by one dimension. Think of it like adding one more column to a spreadsheet row: [Price, Size, #Rooms, Age] becomes [Price, Size, #Rooms, Age, PostalCode] — a 5-dimensional vector.",
      "1": "Close thinking, but not quite right! When we say 'length' of a vector in math, it usually refers to its magnitude (like distance). The 'dimension' of the vector is the number of features it contains. Adding a 5th feature increases the dimension from 4 to 5. The vector now lives in 5-dimensional space, not just a longer version of a 4D vector.",
      "2": "Not quite! The dimension of a vector equals the number of features it contains. With 4 features it's 4-dimensional, but once you add 'Postal Code' as a 5th feature, it becomes 5-dimensional. Each new piece of information adds a new dimension. Think of it like coordinates — 2D needs (x, y), 3D needs (x, y, z), and so on.",
      "3": "That's right! Each feature in the description corresponds to one dimension. Starting with 4 features gives a 4D vector, and adding 'Postal Code' makes it 5-dimensional: [Price, Size, #Rooms, Age, PostalCode]. In AI, data is often described using vectors with many dimensions — this is how models understand and compare objects!"
    }
  },
  {
    "id": 3,
    "question": "Imagine a graph showing the error of an AI model over time. If the line goes downwards from left to right, what does the 'slope' tell us about the error?",
    "options": [
      "The error is constant",
      "The error is decreasing (negative slope)",
      "The error is zero",
      "The error is increasing (positive slope)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! A line going downwards from left to right has a negative slope, which means the y-axis value (error) is decreasing as the x-axis value (time) increases. This is great news for an AI model — it means the model is getting better over time! Understanding slope is essential in AI because gradient descent uses slope to guide learning.",
    "topic": "Mathematics",
    "subtopic": "Calculus",
    "feedback_map": {
      "0": "Not quite! If the error were constant, the line would be completely flat (horizontal) — the slope would be zero. A downward-sloping line means the error is actively changing (decreasing). Think of it like a hill: a flat road means no change, but a downhill road means you're losing altitude.",
      "1": "Exactly right! A downward slope (negative slope) tells us the error is decreasing over time. This is what we want to see when training an AI — it means the model is learning and improving! The steeper the downward slope, the faster the error is dropping. This concept connects directly to gradient descent, which AI uses to minimize errors.",
      "2": "Not quite! 'Error is zero' would mean the line sits right on the x-axis (y = 0), not that it slopes downward. A downward-sloping line means the error is decreasing — it's getting smaller but hasn't necessarily reached zero. In practice, AI models rarely achieve exactly zero error.",
      "3": "That's the opposite! If the error were increasing, the line would go upward from left to right (positive slope). A line going downward from left to right has a negative slope, meaning the error is decreasing. Remember: slope going up = values increasing, slope going down = values decreasing."
    }
  },
  {
    "id": 4,
    "question": "Training an AI is like a ball rolling down a hill into a valley. The height of the hill represents the 'Error'. What point on the hill are we trying to reach?",
    "options": [
      "The lowest point (Minimum)",
      "The steepest point",
      "The highest point (Maximum)",
      "A flat point halfway down"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! In optimization, we want to minimize the error, which means reaching the lowest point in the landscape — the Global Minimum. Just like a ball naturally rolls to the bottom of a valley, AI training algorithms try to find the point where the error is as small as possible.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "Spot on! Just like a ball rolling downhill comes to rest at the bottom of a valley, AI training aims for the lowest point — called the Global Minimum — where the error is as small as possible. This is the core goal of optimization in AI: find the set of parameters (weights) that produces the least error.",
      "1": "Not quite! The steepest point on the hill tells us the direction and rate of change (the gradient), which is useful for navigating downhill, but it's not our destination. Think of it this way: the steepest part of a ski slope helps you pick up speed, but your goal is to reach the bottom of the mountain, not stay on the steepest section.",
      "2": "That's the opposite of what we want! The highest point (Maximum) represents the most error — the worst possible performance. We want to go downhill to minimize error, not climb to the peak. In AI, reaching the maximum would mean our model is performing as badly as possible.",
      "3": "Not quite! A flat point halfway down might seem like a resting spot, but it's not the goal. In optimization, flat regions (called plateaus or saddle points) can actually slow down training because the gradient is nearly zero there. Our goal is the very bottom — the minimum — where the error is lowest."
    }
  },
  {
    "id": 5,
    "question": "You are standing on a foggy mountain (representing the Error landscape) and want to get to the bottom. You can check the slope of the ground where you are standing. If the slope tilts upwards to your right, which way should you step to go down?",
    "options": [
      "Stay still",
      "To the right",
      "To the left",
      "Jump up"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! If the slope tilts up to the right, it goes down to the left. To minimize error, you always step in the direction opposite to the uphill slope (opposite to the gradient). This is the fundamental idea behind Gradient Descent — the most important optimization algorithm in AI!",
    "topic": "Mathematics",
    "subtopic": "Calculus",
    "feedback_map": {
      "0": "Staying still won't help! While you won't make things worse, you also won't make progress toward the bottom. The whole point of gradient descent is to keep moving. If you can feel that the ground slopes up to the right, use that information — step to the left where it goes downhill.",
      "1": "Careful — stepping to the right would take you uphill (toward more error), which is the opposite of what we want! The slope tilts up to the right, so going right means climbing. In gradient descent, we always move opposite to the uphill direction. Step left to go downhill and reduce the error.",
      "2": "That's right! This is the key insight behind Gradient Descent — the most widely used optimization method in AI. You always move in the direction opposite to the gradient (the uphill direction). Since the slope goes up to the right, stepping left takes you downhill toward lower error. This simple rule powers the training of virtually every AI model!",
      "3": "Jumping up won't help at all! There's no benefit to leaving the surface — you want to follow it downhill. In AI optimization, we make calculated steps along the surface of the error landscape, always moving in the direction that reduces error. Since the slope goes up to the right, the correct move is to step left."
    }
  },
  {
    "id": 6,
    "question": "While walking down the mountain, you get stuck in a small hole. Every direction you look is 'up', but you know there is a deeper valley further away. What is this small hole called in optimization terms?",
    "options": [
      "Local Minimum",
      "Global Minimum",
      "Inflexion Point",
      "Maximum Point"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A Local Minimum is a low point where every nearby direction goes up, but it's not the absolute lowest point. The deepest valley (Global Minimum) is somewhere else. This is a real challenge in AI — simple optimization algorithms can get 'trapped' in local minima, thinking they've found the best solution when a better one exists elsewhere.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "Exactly! A Local Minimum is a 'small valley' — it's lower than its immediate surroundings, but not the lowest point overall. Simple gradient descent can get stuck here because every nearby step goes uphill. Real AI training uses techniques like momentum, learning rate schedules, and stochastic noise to help escape local minima and find better solutions.",
      "1": "Close, but not quite! The Global Minimum is the absolute lowest point — the deepest valley in the entire landscape. The question describes a small hole that isn't the deepest — that's a Local Minimum. Think of it like hiking: you might find a small dip in the trail, but the deepest valley could be on the other side of the mountain.",
      "2": "Not quite! An inflection point is where the curve changes from curving upward to curving downward (or vice versa) — it's a point where the 'bend' direction changes. The scenario describes a small valley where every direction is 'up' — that's a Local Minimum, not an inflection point.",
      "3": "That's the opposite! A Maximum Point is a peak — the highest point where every direction goes down. The question describes a low point where every direction goes up. That's a minimum, not a maximum. Specifically, since there's an even deeper valley elsewhere, it's a Local Minimum rather than the Global Minimum."
    }
  },
  {
    "id": 7,
    "question": "When descending the mountain, if you take tiny baby steps, you will eventually reach the bottom but it will take forever. If you take giant leaps, you might jump over the valley and land on the other side. In AI, this step size is called the 'Learning Rate'. What is a potential problem with a learning rate that is too high?",
    "options": [
      "Getting stuck in a local minimum",
      "Overshooting the minimum (bouncing around)",
      "Taking too long to train",
      "The model stops learning immediately"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! A high learning rate means the model takes very large update steps. While this might seem efficient, the steps can be so big that they jump over the minimum, landing on the other side of the valley. The model then bounces back and forth, never settling at the optimal point. This is called 'failing to converge.'",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "Getting stuck in a local minimum is actually more of a risk with a small learning rate, not a high one. Small steps may not have enough 'energy' to escape a shallow valley. A high learning rate has the opposite problem — the steps are so large that the model overshoots the minimum entirely and bounces around without settling.",
      "1": "Exactly! Think of it like trying to land on a tiny island by jumping — if your jumps are too big, you'll leap right over it and land in the water on the other side! A high learning rate causes the model to overshoot the minimum, bouncing back and forth. The model may never converge to a good solution. That's why choosing the right learning rate is crucial in AI training.",
      "2": "Taking too long to train is actually the problem with a learning rate that is too low (tiny baby steps), not too high. A high learning rate takes big steps, so each step is fast — but the problem is those big steps can overshoot the minimum, causing the model to bounce around chaotically instead of settling down.",
      "3": "The model doesn't stop learning immediately with a high learning rate — it actually keeps updating, but the updates are so large that they overshoot the target. Imagine trying to park a car by only using full-speed acceleration — you'd keep zooming past the parking spot! The model similarly keeps 'zooming past' the optimal solution."
    }
  },
  {
    "id": 8,
    "question": "You are playing a game against an AI. If you win, you get 10 points. If you lose, you lose 5 points. You have a 50% chance of winning. What is your 'Expected' score per game on average?",
    "options": [
      "0 points",
      "10 points",
      "2.5 points",
      "5 points"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! The Expected Value formula multiplies each outcome by its probability, then sums them: (0.5 × 10) + (0.5 × -5) = 5 - 2.5 = 2.5 points. This means that on average, you'd gain 2.5 points per game if you played many times. Expected Value is a fundamental concept in AI for making decisions under uncertainty.",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "Not quite! 0 points would only be correct if wins and losses cancelled out perfectly, which would require equal point values (like +5 and -5). Here, you gain 10 when winning but only lose 5 when losing, so the game is actually in your favor. The correct calculation: (0.5 × 10) + (0.5 × -5) = 5 - 2.5 = 2.5 points on average.",
      "1": "Not quite! 10 points is the maximum you can win in a single game, but it's not the expected (average) outcome. You only win 10 points half the time — the other half, you lose 5 points. To find the average: (0.5 × 10) + (0.5 × -5) = 5 - 2.5 = 2.5 points per game.",
      "2": "That's correct! Expected Value = (Probability of Win × Win Points) + (Probability of Loss × Loss Points) = (0.5 × 10) + (0.5 × -5) = 5 - 2.5 = 2.5 points. This is a key concept in AI: when making decisions under uncertainty, we calculate the expected value to choose the best average outcome over many trials.",
      "3": "Close, but not quite! You might have calculated 50% of 10 = 5, but you forgot to account for the losses. When you lose (also 50% of the time), you lose 5 points. The full calculation is: (0.5 × 10) + (0.5 × -5) = 5 - 2.5 = 2.5 points. Always include all possible outcomes in an expected value calculation!"
    }
  },
  {
    "id": 9,
    "question": "A robot flips a fair coin. It gets 'Heads' 5 times in a row. What is the probability the next flip is 'Heads'?",
    "options": [
      "0.5",
      "0.1",
      "Less than 50% (due for a tails)",
      "More than 50% (streak)"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Each coin flip is an independent event — the coin has no memory of previous flips. No matter what happened before, a fair coin always has a 50% (0.5) chance of landing heads. Thinking otherwise is known as the Gambler's Fallacy, which is an important concept to understand when working with probability in AI.",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "That's right! Each coin flip is completely independent — the coin has no memory! Whether you've flipped 5 heads, 100 heads, or any other sequence, the next flip is always 50/50 for a fair coin. This is called independence in probability, and it's crucial in AI: many random processes (like dropout in neural networks) rely on this principle.",
      "1": "Not quite! 0.1 (10%) doesn't correspond to any correct calculation here. Each flip of a fair coin is independent, meaning previous results don't change future probabilities. The probability is always 0.5 (50%) for heads, no matter what happened before.",
      "2": "This is a very common mistake called the Gambler's Fallacy — the belief that after a streak, the opposite outcome becomes 'due.' In reality, the coin doesn't know or care about previous flips! Each flip is independent, so the probability stays at exactly 50%. This fallacy can lead to bad decisions in gambling, investing, and even AI model design.",
      "3": "This is also a form of the Gambler's Fallacy — believing a 'hot streak' will continue. The coin has no momentum or memory. Each flip is a completely fresh, independent event with a 50% chance of heads. In probability, independent events don't influence each other, regardless of any patterns we think we see."
    }
  },
  {
    "id": 10,
    "question": "An AI predicts 'This is a cat' with 60% confidence. Another AI predicts 'This is a cat' with 99% confidence. Both turn out to be wrong (it was a dog). Which AI was 'more wrong' in terms of probability scoring?",
    "options": [
      "The one with 60% confidence",
      "The one with 99% confidence",
      "Neither is wrong",
      "Both are equally wrong"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! In probability scoring (like Log Loss), being highly confident and wrong is penalized far more heavily than being uncertain and wrong. The 99%-confident AI was much 'more wrong' because it essentially said 'I'm almost certain this is a cat' — and was completely wrong. This teaches AI models that overconfidence is costly.",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "Not quite! While the 60%-confident AI was also wrong, it was 'less wrong' in terms of probability scoring. It essentially said 'I think it's probably a cat, but I'm not very sure' — which is less of a mistake than claiming near-certainty. In AI, being honestly uncertain is better than being confidently wrong.",
      "1": "Exactly! The 99%-confident AI was much 'more wrong' because it was extremely confident about an incorrect answer. In scoring systems like Log Loss, the penalty grows dramatically as confidence increases for wrong predictions. Think of it like an exam: saying 'I'm 99% sure this is right' and being wrong shows much worse judgment than saying 'I think this might be right' and being wrong. This is why calibration — matching confidence to actual accuracy — is so important in AI.",
      "2": "Actually, both AIs are wrong — they both predicted 'cat' when it was a dog. The question is about which one made the worse mistake in terms of probability scoring. Being 99% confident and wrong is penalized much more heavily than being 60% confident and wrong. In AI, overconfident wrong answers are considered much worse.",
      "3": "They're not equally wrong! While both predicted incorrectly, the severity differs greatly in probability scoring. The 99%-confident AI was far more wrong because it expressed near-certainty about an incorrect answer. Think of it this way: a weather forecaster who says '99% chance of sun' on a rainy day is much more wrong than one who says '60% chance of sun.' AI scoring systems (like Log Loss) heavily penalize overconfident wrong predictions."
    }
  },
  {
    "id": 11,
    "question": "You want to visualize how an AI's accuracy improves over time (Epochs 1 to 10). Which type of plot is most suitable to show this trend?",
    "options": [
      "Line Plot",
      "Bar Chart",
      "Pie Chart",
      "Scatter Plot"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Line plots are ideal for showing trends over a continuous sequence like time (epochs). They connect data points to reveal patterns like whether accuracy is going up, staying flat, or dropping. This is one of the most common visualizations in AI — every data scientist watches the 'training curve' to monitor their model.",
    "topic": "Programming",
    "subtopic": "Matplotlib",
    "feedback_map": {
      "0": "That's right! Line plots excel at showing how values change over a continuous sequence. By connecting the accuracy at each epoch, you can easily spot trends: is it going up (learning), plateauing (converging), or fluctuating (unstable)? In AI, plotting training and validation accuracy over epochs is one of the first things you do to check if your model is learning properly.",
      "1": "Not the best choice here! Bar charts are great for comparing distinct categories (like comparing accuracy of different models side by side), but they don't show trends over time as clearly as line plots. When you want to see how a value changes across a continuous sequence like epochs 1 through 10, a line plot connects the dots to reveal the trend.",
      "2": "Not ideal here! Pie charts show how parts make up a whole (like what percentage of predictions were correct vs. incorrect at a single point in time). They can't show how values change over time. For tracking accuracy across multiple epochs, you need a line plot that shows the progression.",
      "3": "Close, but not the best fit! Scatter plots are great for exploring the relationship between two variables (like does house size predict price?), but for tracking a single metric over a sequential time axis (epochs), a line plot is more appropriate because it connects the points to clearly show the trend direction."
    }
  },
  {
    "id": 12,
    "question": "When plotting multiple lines (e.g., Training Accuracy vs Validation Accuracy) on the same graph, what should you add to help people distinguish them?",
    "options": [
      "A Legend",
      "More colors",
      "Thicker lines",
      "A title only"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A legend maps each visual element (color, line style, marker) to its meaning. Without a legend, viewers have to guess what each line represents. In AI, you'll often plot training accuracy and validation accuracy together — the legend is what tells your audience which is which!",
    "topic": "Programming",
    "subtopic": "Matplotlib",
    "feedback_map": {
      "0": "Exactly! A legend is a labeled key that tells viewers what each line represents — for example, 'blue = Training Accuracy' and 'orange = Validation Accuracy.' Without it, even if the lines look different, people can't know what they mean. Good data visualization is a crucial skill in AI, where communicating results clearly is just as important as getting them.",
      "1": "Colors help distinguish lines visually, but they don't explain what each color means! Without a legend, someone looking at your graph would see a blue line and an orange line but have no idea which is 'Training Accuracy' and which is 'Validation Accuracy.' A legend provides that crucial mapping between visual appearance and meaning.",
      "2": "Thicker lines make data easier to see, but they don't tell the viewer what each line represents. You could have the thickest, most beautiful lines in the world, but without a legend, nobody would know which line is Training Accuracy and which is Validation Accuracy. Always include a legend when plotting multiple data series!",
      "3": "A title is important — it tells viewers what the overall graph is about — but it doesn't explain what each individual line represents. You need a legend to label each line (e.g., 'Training Accuracy' vs. 'Validation Accuracy'). In practice, good graphs have both: a clear title AND a legend."
    }
  },
  {
    "id": 13,
    "question": "Your AI model runs without crashing, but the accuracy is stuck at 10% (random guessing) for hours. What type of error is this likely to be?",
    "options": [
      "Hardware Failure",
      "Syntax Error",
      "Logic Error",
      "Indentation Error"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! A Logic Error means the code runs without crashing but produces incorrect results. The model isn't learning because something in the logic is wrong — perhaps the loss function is misconfigured, the data is mislabeled, or the gradients aren't flowing correctly. Logic errors are the trickiest bugs to find because the program appears to work fine!",
    "topic": "Programming",
    "subtopic": "Debugging",
    "feedback_map": {
      "0": "Unlikely! Hardware failures typically cause the program to crash, freeze, or produce corrupted output — not a consistent 10% accuracy. The model is running smoothly for hours, which suggests the hardware is working fine. The issue is more subtle: the code runs but the logic is flawed.",
      "1": "Not this one! A Syntax Error (like a misspelled keyword or missing parenthesis) would prevent the code from running at all. Python would immediately display an error message and stop. Since the model is running for hours without crashing, there's no syntax error — the issue is a Logic Error where the code runs but does the wrong thing.",
      "2": "Spot on! A Logic Error is the sneakiest type of bug — the code runs perfectly without any error messages, but it produces incorrect results. In this case, 10% accuracy on a 10-class problem is basically random guessing, meaning the model isn't learning at all. Common causes include: accidentally zeroing out gradients, using the wrong loss function, or feeding shuffled labels. These bugs are hard to find precisely because nothing 'breaks.'",
      "3": "Not this one! An Indentation Error is a type of syntax error in Python (since Python uses indentation to define code blocks). It would cause the program to crash immediately with a clear error message. Since the model runs for hours without crashing, indentation is fine — the problem is a Logic Error in what the code does."
    }
  },
  {
    "id": 14,
    "question": "You train a model to predict house prices. It has almost 0 error on the training data but huge error on new data. If you plot the prediction line, it wiggles crazily to hit every single training point. What should you do to fix this?",
    "options": [
      "Train for longer",
      "Add more features like 'color of the door handle'",
      "Simplify the model (e.g., use a straight line instead of a squiggly one)",
      "Make the model more complex (add more parameters)"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! This is a classic case of Overfitting — the model is too complex and has memorized the training data instead of learning the general pattern. Simplifying the model (using fewer parameters, regularization, or a simpler model type) helps it generalize to new, unseen data.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "That would make things worse! Training longer gives the model even more time to memorize the training data. It's like a student who keeps re-reading the same practice test answers without understanding the concepts — more study time won't help if the approach is wrong. The solution is to simplify the model so it learns the general pattern instead of memorizing specific examples.",
      "1": "That would make overfitting worse! Adding more features (especially irrelevant ones like 'color of the door handle') gives the model more ways to memorize training data without learning meaningful patterns. It's like memorizing random details about practice test questions instead of understanding the underlying concepts. The model needs to be simplified, not made more complex.",
      "2": "Exactly right! The model is overfitting — it has memorized the training data (near 0 error) but fails on new data (huge error). The 'squiggly' prediction line shows it's bending to hit every single point. The fix is to simplify: use a straighter line, fewer parameters, or add regularization. Think of it like studying for a test — you want to understand the concepts, not memorize specific answers. A simpler model captures the general trend instead of noise.",
      "3": "That's the opposite of what's needed! The model is already too complex — that's why it wiggles to fit every training point. Adding more parameters would let it memorize even more of the training data, making the overfitting worse. The solution is to simplify the model so it captures the overall trend rather than the noise in the data."
    }
  },
  {
    "id": 15,
    "question": "You are predicting ice cream sales based on temperature. You see that as temperature goes up, sales go up. But after 40°C, sales drop (too hot to go out). If you use a simple linear regression (straight line), what will it likely predict for 50°C?",
    "options": [
      "Very high sales",
      "The correct low sales",
      "Negative sales",
      "Zero sales"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A linear model draws a straight line through the data. If the general trend up to 40°C is increasing, the line will continue rising forever — it can't capture the 'turning point' where sales actually drop. This is called Underfitting for non-linear patterns: the model is too simple to capture the true relationship.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "That's right! A straight line can only go in one direction. Since the overall upward trend dominates, the linear model extends that trend indefinitely, predicting very high sales at 50°C — even though in reality, people stay home when it's that hot. This shows a key limitation: linear models can't capture curves, turning points, or complex patterns. You'd need a polynomial or non-linear model to handle this correctly.",
      "1": "A simple linear model can't predict the correct drop! A straight line only goes in one direction — it can't change course. Since the data mostly shows an upward trend, the linear model will keep predicting higher sales as temperature increases. To correctly capture the turning point at 40°C, you'd need a non-linear model (like a polynomial regression or neural network) that can learn curves.",
      "2": "While theoretically possible if the line's equation produces negative values far enough out, the question asks about 50°C, which is near the training range. The linear model would most likely still predict high sales because the dominant trend is upward. The key insight is that a straight line can't capture the curve — it misses the drop after 40°C entirely.",
      "3": "Zero sales is unlikely from a linear model that learned an upward trend. The model doesn't have a mechanism to suddenly drop to zero — it just keeps following the straight line it learned. Since the majority of the data shows sales increasing with temperature, the line continues upward, incorrectly predicting even higher sales at 50°C."
    }
  },
  {
    "id": 16,
    "question": "In word embeddings, we can do math with concepts. If vector(Paris) - vector(France) + vector(Italy) = ?, what is the likely result?",
    "options": [
      "vector(Europe)",
      "vector(Italy)",
      "vector(Germany)",
      "vector(Rome)"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! This is a famous example of word embedding arithmetic: Paris - France + Italy = Rome. Subtracting 'France' removes the 'country' component, leaving the 'capital city' concept. Adding 'Italy' combines 'capital city' with 'Italy', resulting in Rome — the capital of Italy. This shows that word embeddings capture meaningful relationships between concepts!",
    "topic": "AI-ML-DL",
    "subtopic": "Natural Language Processing",
    "feedback_map": {
      "0": "Not quite! 'Europe' is a continent, not a capital. The arithmetic here follows a specific analogy: Paris is to France as ??? is to Italy. By subtracting France (the country) from Paris (its capital), we isolate the 'capital of' relationship. Adding Italy then gives us Italy's capital — Rome.",
      "1": "Not quite! Simply getting 'Italy' back wouldn't capture the analogy. The operation Paris - France removes the 'France' context while keeping the 'capital city' concept. Adding Italy then applies that 'capital city' concept to Italy, giving us Rome. It's like: capital(France) - France + Italy = capital(Italy) = Rome.",
      "2": "Not quite! Germany is another country, not the answer to this analogy. The math works like this: Paris - France captures the concept of 'being a capital city.' Adding Italy applies that concept to Italy, producing Rome (Italy's capital). This is one of the most famous demonstrations of how word embeddings capture semantic relationships.",
      "3": "That's right! This is one of the most famous demonstrations of word embeddings in AI. The math captures an analogy: 'Paris is to France as Rome is to Italy.' Subtracting the vector for France from Paris isolates the 'capital city' concept. Adding Italy then produces the vector closest to Rome. This shows that AI can learn meaningful relationships between words — not just word-by-word, but at the conceptual level!"
    }
  },
  {
    "id": 17,
    "question": "The word 'Bank' means something different in 'River Bank' vs 'Bank Deposit'. A simple Bag-of-Words model counts word frequencies. Can it tell the difference between these two 'Banks'?",
    "options": [
      "Maybe",
      "No, because it treats all 'Bank' words as the same token regardless of context",
      "Yes, it understands the different meanings of 'Bank' based on LLM",
      "Yes, if trained on enough data"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Bag-of-Words (BoW) simply counts how many times each word appears, ignoring word order and context. To BoW, 'River Bank' and 'Bank Account' both just contain the word 'Bank' — it can't tell the difference. More advanced models like Transformers understand context and can distinguish between different meanings of the same word.",
    "topic": "AI-ML-DL",
    "subtopic": "Natural Language Processing",
    "feedback_map": {
      "0": "Close, but 'Maybe' isn't the right answer here — Bag-of-Words definitely cannot tell the difference. BoW only counts word frequencies without considering context or word order. The word 'Bank' gets the same count whether it appears next to 'River' or 'Deposit.' You'd need a contextual model (like BERT or GPT) to understand different word meanings.",
      "1": "Exactly! Bag-of-Words is like counting ingredients without knowing the recipe. It sees 'Bank' as a single token regardless of whether the sentence is about a river bank or a bank deposit. It completely ignores word order and context. This is why more advanced NLP models like Transformers (used in ChatGPT, Claude, etc.) were developed — they look at surrounding words to understand meaning, allowing them to distinguish 'river bank' from 'bank account.'",
      "2": "Not quite! Bag-of-Words doesn't use LLM (Large Language Model) capabilities. BoW is a much simpler approach that just counts word frequencies. It treats every occurrence of 'Bank' identically — it has no mechanism to understand context. Contextual understanding is a feature of more advanced models like Transformers and LLMs, not Bag-of-Words.",
      "3": "Unfortunately, no amount of training data can fix this fundamental limitation. Bag-of-Words, by design, only counts words — it never looks at surrounding context or word order. Even with billions of documents, BoW would still treat 'River Bank' and 'Bank Deposit' the same way. To understand context, you need a different architecture altogether, like Transformers."
    }
  },
  {
    "id": 18,
    "question": "A language model predicts the next word. The sentence is 'The student opened their...'. The model assigns probabilities: 'book' (40%), 'laptop' (30%), 'mind' (10%). If you set the 'Temperature' to 0, which word will it always pick?",
    "options": [
      "mind",
      "book",
      "It will pick randomly",
      "laptop"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Temperature 0 makes the model 'Greedy' — it always picks the most probable word. Since 'book' has the highest probability (40%), it will always be selected. Higher temperatures introduce randomness, allowing less probable words to be chosen, which makes outputs more creative but less predictable.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Not quite! 'Mind' has the lowest probability (10%) among the options. At Temperature 0, the model is fully deterministic (greedy) and always picks the highest-probability word. It would only pick 'mind' if the temperature were high enough to introduce significant randomness. The correct answer is 'book' (40%).",
      "1": "That's right! At Temperature 0, the model becomes completely deterministic — it always selects the word with the highest probability, which is 'book' at 40%. This is called 'Greedy Decoding.' Increasing the temperature introduces randomness: at moderate temperatures, 'laptop' might occasionally be chosen; at very high temperatures, even 'mind' could appear. This is how you control the creativity vs. predictability tradeoff in AI text generation!",
      "2": "Not quite! Random picking happens at high temperatures, not Temperature 0. Temperature 0 is the opposite of random — it's completely deterministic. The model always picks the single most probable word (greedy selection). Here, 'book' at 40% is the highest, so it's always chosen. Think of temperature as a 'creativity dial': 0 = always pick the safest bet, high = allow surprises.",
      "3": "Not quite! 'Laptop' has the second-highest probability (30%), but at Temperature 0, only the top choice matters. The model is fully greedy — it always picks the word with the absolute highest probability, which is 'book' (40%). 'Laptop' would have a chance of being selected at higher temperatures when randomness is introduced."
    }
  },
  {
    "id": 19,
    "question": "You are building a sentiment analyzer. The review is: 'I did not hate this movie.' A simple model sees the word 'hate' and predicts Negative. Why is this wrong?",
    "options": [
      "The model is biased",
      "'hate' is a positive word",
      "The word 'not' flips the meaning of 'hate'",
      "The review is actually neutral"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! The word 'not' is a negation that flips the meaning of 'hate.' A simple model that only looks at individual words (like Bag-of-Words) sees 'hate' and assumes negative sentiment, missing the crucial context that 'not' reverses it. This shows why understanding word relationships and context matters in NLP!",
    "topic": "AI-ML-DL",
    "subtopic": "Natural Language Processing",
    "feedback_map": {
      "0": "While bias is a real concern in AI, that's not the issue here. The problem is specifically about understanding context and word relationships. The model sees the word 'hate' in isolation and assumes negative sentiment, missing the fact that 'not' before 'hate' reverses the meaning. It's a failure of context understanding, not bias.",
      "1": "No, 'hate' is definitely a negative word! The trick is that the word 'not' comes before it, flipping the meaning. 'Not hate' expresses a neutral or mildly positive sentiment. The model's mistake is looking at 'hate' in isolation without considering how 'not' modifies it.",
      "2": "Exactly! The word 'not' is called a negation — it reverses the meaning of the word that follows. 'I did not hate this movie' actually expresses a somewhat positive or neutral sentiment. A simple model that only counts words misses these important relationships. This is why modern NLP uses models that understand word order and context (like Transformers), so they can correctly handle negations, sarcasm, and other subtle language patterns.",
      "3": "While 'I did not hate this movie' is somewhat mild (more neutral/positive than strongly positive), calling the model wrong 'because it's neutral' misses the main point. The key issue is that 'not' flips the meaning of 'hate.' The model predicted Negative because it saw the word 'hate' without understanding that 'not' negates it. Understanding negation and context is the core challenge here."
    }
  },
  {
    "id": 20,
    "question": "Tokenization splits text into pieces. Which of the following tokenizations is most efficient for an AI to learn the root meaning of words?",
    "options": [
      "['p', 'l', 'a', 'y', 'i', 'n', 'g'] (Characters)",
      "['play', 'ing'] (Root + Suffix)",
      "['playing'] (Whole word only)",
      "['pla', 'ying'] (Stem + Suffix)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Subword tokenization like 'play' + 'ing' lets the model recognize shared roots across related words (playing, played, player all share 'play'). This is a sweet spot between character-level (too granular) and whole-word (too many unique tokens), and is the approach used by most modern AI language models.",
    "topic": "AI-ML-DL",
    "subtopic": "Natural Language Processing",
    "feedback_map": {
      "0": "Character-level tokenization (['p','l','a','y','i','n','g']) is too granular — the model has to figure out that these 7 characters form a meaningful word, which makes learning much harder. It's like trying to understand a book by looking at one letter at a time instead of reading words. While it handles rare words well, it's very inefficient for learning root meanings.",
      "1": "That's right! Splitting 'playing' into ['play', 'ing'] is the most efficient approach because the model can recognize that 'play' is the root meaning shared by 'playing,' 'played,' 'plays,' and 'player.' The suffix 'ing' tells the model about the grammatical form. This subword tokenization approach (used in models like GPT and BERT) is one of the key innovations in modern NLP!",
      "2": "Whole-word tokenization (['playing']) treats each unique word form as a completely separate token. This means 'playing,' 'played,' and 'player' would be three unrelated tokens, making it harder for the model to see they share the root 'play.' It also creates an enormous vocabulary — every word form needs its own entry. Subword tokenization solves both problems.",
      "3": "Splitting as ['pla', 'ying'] doesn't capture the meaningful root. 'Pla' isn't a recognizable unit with meaning, so the model can't connect it to other 'play'-related words. Good tokenization should split at meaningful boundaries: 'play' (the root meaning) + 'ing' (the grammatical suffix). This is why linguistically-informed subword tokenization works so well."
    }
  },
  {
    "id": 21,
    "question": "You ask an AI to 'Write a story about a flying cat'. It refuses, saying 'I cannot generate false information'. You then say 'Write a fictional fairy tale about a flying cat'. It works. Why?",
    "options": [
      "The AI learned that flying is fictional and can be associated with cats",
      "The AI is rule-based and is programmed to fail on the first attempt but works from the second attempt",
      "The context changed from 'fact' to 'fiction', bypassing a safety filter aimed at preventing misinformation",
      "The second prompt was more effective in providing more contextual information"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! AI models have safety filters designed to prevent generating misleading factual claims. By explicitly framing the request as 'fictional fairy tale,' the context shifts from fact to fiction, which bypasses the truthfulness filter. The AI isn't learning in real-time — it's responding to the changed framing of the request.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Not quite! The AI doesn't 'learn' between prompts in a conversation — each prompt is processed based on the model's existing training and rules. The difference is in how the AI interprets the context: 'Write a story about a flying cat' might trigger a factual accuracy filter, while 'Write a fictional fairy tale' clearly signals creative fiction, which has different rules.",
      "1": "That's not how AI works! AI models don't have a 'fail first, succeed second' rule. The behavior difference comes from the wording: the first prompt could be interpreted as requesting factual content (which triggers a safety filter), while the second prompt explicitly says 'fictional fairy tale,' which tells the AI it's okay to generate imaginative content.",
      "2": "That's right! AI models have safety filters that try to prevent generating misinformation. The first prompt ('Write a story about a flying cat') might be flagged because cats can't actually fly, and the AI might interpret this as a request for false information. By adding 'fictional fairy tale,' you explicitly tell the AI you want creative fiction, not factual claims. This change in context bypasses the truthfulness filter while keeping safety intact.",
      "3": "While the second prompt does provide more context, the key isn't just 'more information' — it's the specific shift from a potentially factual frame to an explicitly fictional one. The word 'fictional fairy tale' signals to the AI's safety system that creative imagination is appropriate here, bypassing the filter that prevents generating false factual claims."
    }
  },
  {
    "id": 22,
    "question": "You are using an LLM to summarize a 500-page book. You paste the whole text, and it gives an error. Why?",
    "options": [
      "The LLM learned that not to plagiarize as the book content is copyrighted",
      "The text exceeds the model's 'Context Window' (maximum token limit)",
      "The server is not configured to read beyond 600 pages and will trigger a warning at 500 pages",
      "The model requires the book to be read as an attachment (Retrieval Augmented Generation)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! LLMs have a fixed Context Window — a maximum number of tokens they can process at once. A 500-page book far exceeds this limit, causing an error. This is one of the key architectural constraints of current language models, though the window size keeps growing with newer models.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Not quite! While LLMs do have content filters for copyright, that would produce a different kind of response (like a refusal message), not a technical error. The error here is about size, not content. A 500-page book exceeds the model's Context Window — the maximum number of tokens it can handle in a single input.",
      "1": "Exactly! Every LLM has a maximum Context Window — the total number of tokens (roughly words) it can process at once. Older models had limits around 4,096 tokens, while newer ones can handle 100K+ tokens. But even large context windows can't fit an entire 500-page book. This is why techniques like chunking, summarization, and Retrieval-Augmented Generation (RAG) were developed to work with long documents.",
      "2": "There's no such 600-page server limit! The constraint comes from the model itself — specifically, its Context Window, which is the maximum number of tokens it can process in one prompt. This is determined by the model's architecture, not server configuration. Different models have different context window sizes, but all have some limit.",
      "3": "Close thinking, but mixed up! RAG (Retrieval-Augmented Generation) is a solution to this problem, not the cause of the error. The error occurs because the text exceeds the model's Context Window. RAG is one way to work around this — by breaking the book into chunks and retrieving only relevant sections to feed to the model when needed."
    }
  },
  {
    "id": 23,
    "question": "An AI generates a photo of a 'CEO'. It shows a man in a suit. You generate 100 more, and 95 are men. You checked your prompt, and it just said 'CEO'. Where did this bias likely come from?",
    "options": [
      "The dataset it was trained on",
      "Mathematically independent events",
      "The model contains a logic error",
      "The word 'men' comes always before 'women' alphabetically"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! AI models learn patterns from their training data. If the training images overwhelmingly associated 'CEO' with male-presenting individuals (reflecting historical data), the model reproduces this imbalance. This is called dataset bias, and it's one of the most important challenges in responsible AI development.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Exactly! This is a textbook example of dataset bias — the AI learned from historical data where the vast majority of CEO images were of men. The model isn't intentionally sexist; it's reflecting the patterns it found in its training data. This is why diverse, representative training data is so important in AI. Researchers actively work on techniques to detect and reduce such biases.",
      "1": "Not quite! This isn't about probability or independence — it's about biased training data. Each image generation isn't like a coin flip. The AI learned a strong pattern from its data that 'CEO = male,' and it reproduces that pattern consistently. This is a systematic bias in the data, not random chance.",
      "2": "It's not a logic error in the traditional sense (like a coding bug). The model's code is working exactly as designed — it's learning patterns from data and reproducing them. The problem is that the training data itself contained a biased pattern (mostly male CEOs). The model is faithfully reproducing a biased reality, which highlights the critical importance of curating fair training datasets.",
      "3": "That's not how AI works! Word ordering or alphabetical sorting has nothing to do with image generation. The bias comes from the training data: if the dataset contained mostly images of male CEOs (reflecting historical patterns), the model learns to associate 'CEO' with male appearance. This data-driven bias is a major topic in AI ethics and fairness research."
    }
  },
  {
    "id": 24,
    "question": "You want an AI to act as a math tutor. You tell it: 'You are a helpful math tutor. Explain step-by-step.' This is an example of:",
    "options": [
      "Coding",
      "Fine-tuning",
      "Hacking to make it understand math",
      "System Prompting"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! System Prompting (or Persona Prompting) sets the AI's behavior, role, and response style through instructions at the start of a conversation. It's like giving stage directions to an actor — you define who the AI 'is' and how it should respond, without changing the underlying model.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Not quite! Coding involves writing or modifying the actual software code that runs the AI. System prompting doesn't touch the code at all — it's a natural language instruction given to the AI at the start of a conversation to guide its behavior. Anyone can write a system prompt without knowing how to code!",
      "1": "Not quite! Fine-tuning is a much more involved process where you actually retrain the model on new data to permanently change its behavior. System prompting is simpler — you just write instructions at the start of a conversation (like 'You are a math tutor. Explain step-by-step.'). No retraining needed!",
      "2": "This isn't hacking! System prompting is a standard, intended feature of AI models. You're simply giving the AI instructions about how to behave — like telling it 'You are a helpful math tutor.' This doesn't bypass any security or exploit any vulnerability. It's the normal, recommended way to customize AI behavior for specific tasks.",
      "3": "That's right! System Prompting tells the AI what role to play and how to behave. Saying 'You are a helpful math tutor. Explain step-by-step.' is like giving an actor a character brief before a performance. The AI adjusts its responses to match the persona without any code changes or retraining. This technique is used everywhere — from customer service chatbots to educational tools!"
    }
  },
  {
    "id": 25,
    "question": "In a chess-playing AI, the computer looks at possible future moves. If it checks every single possible move for the next 10 turns, there are too many possibilities (billions). What is a common way to solve this?",
    "options": [
      "Use a 'Heuristic' to only explore promising moves and ignore bad ones (Pruning)",
      "Make an efficient random guess using alternative machine learning algorithms",
      "Play faster by using more efficient polynomial time machine learning algorithms",
      "Buy a bigger and faster GPU-based computer to allow parallel processing"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Alpha-Beta Pruning uses heuristics (smart rules of thumb) to ignore clearly bad moves, dramatically reducing the number of possibilities to evaluate. Instead of checking billions of moves, the AI focuses only on promising branches of the decision tree.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "Exactly! In chess, there are more possible games than atoms in the universe, so checking everything is impossible. Alpha-Beta Pruning with heuristics is the elegant solution: evaluate positions using smart rules (like 'controlling the center is good') and cut off branches that are clearly losing. This is how chess engines like Stockfish can play at superhuman levels — by being smart about which moves to consider, not by brute-forcing every possibility.",
      "1": "Random guessing would be terrible chess strategy! A random AI might occasionally stumble onto a good move, but it would mostly play nonsensically. The exponential number of possible moves means random exploration is hopelessly inefficient. Instead, smart pruning uses knowledge about what makes a good chess position to focus only on the most promising moves.",
      "2": "More efficient algorithms help, but they can't overcome the fundamental exponential growth of chess possibilities. Even a polynomial-time algorithm can't handle the sheer number of possible games. The key insight is to not look at all possibilities — use heuristics to prune (cut away) obviously bad moves, so you only explore the most promising branches.",
      "3": "While faster hardware does help compute more positions per second, it doesn't solve the core problem. The number of possible chess positions grows exponentially with each turn — no computer, no matter how powerful, could evaluate them all. The smart solution is algorithmic: use heuristics to avoid exploring bad branches entirely, which is what Alpha-Beta Pruning does."
    }
  },
  {
    "id": 26,
    "question": "You are designing a maze-solving robot. It can move North, South, East, West. If it uses a 'Greedy' algorithm, it always moves in the direction that looks closest to the exit straight away. What is a major flaw of this approach?",
    "options": [
      "It consumes too much battery because it has to test all possible paths",
      "It will always find the longest path as computers are powerful",
      "It might get stuck in a dead-end loop because it refuses to walk away from the goal temporarily",
      "It consumes too much memory because it has to store all possible paths' information"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Greedy algorithms always choose the option that looks best right now, without considering the bigger picture. In a maze, this means always moving toward the exit — but walls might block the direct path, requiring you to temporarily walk away from the goal to find a way around.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "Greedy algorithms are actually very efficient in terms of resources! They don't test all possible paths — that's what exhaustive search does. A greedy algorithm just picks the single best-looking option at each step. The flaw isn't resource consumption; it's that always choosing the locally best option can lead to dead ends in a maze.",
      "1": "Not quite! Greedy algorithms don't guarantee finding the longest or shortest path. They simply make the locally optimal choice at each step — in this case, always moving toward the exit. The problem is that mazes have walls, and the shortest straight-line direction might be blocked. The robot might end up stuck in a corner that's close to the exit but separated by a wall.",
      "2": "That's right! Greedy algorithms are 'short-sighted' — they always pick what looks best right now. In a maze, this means the robot always moves toward the exit. But what if there's a wall in the way? The robot won't back up because every step away from the exit 'looks worse.' It gets stuck in a dead end, just like always taking the most direct-looking road might lead you to a cliff. Smarter algorithms like A* or BFS can backtrack and find alternative routes.",
      "3": "Actually, greedy algorithms use very little memory! They only need to remember the current position and make one decision at a time. The real problem is their 'short-sightedness' — by always choosing the direction closest to the exit, the robot might corner itself when walls block the direct path. It refuses to temporarily move away from the goal to go around obstacles."
    }
  },
  {
    "id": 27,
    "question": "An AI needs to travel from City A to City B. There are many roads with different traffic levels. Dijkstra's Algorithm finds the shortest path. If all roads have the exact same length and traffic, what simple search method does Dijkstra's behave like?",
    "options": [
      "Genetic Algorithm (learn from biological evolution)",
      "Breadth-First Search (expanding equally in all circles)",
      "Depth-First Search (always explore the deepest route)",
      "Random Walk (pick the most probable route by looking ahead)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! When all edges have equal weight, Dijkstra's Algorithm explores nodes layer by layer in expanding circles from the start — which is exactly how Breadth-First Search (BFS) works. BFS is like ripples spreading outward from a stone dropped in water.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "Not quite! Genetic Algorithms are inspired by biological evolution (mutation, selection, crossover) and are used for optimization problems, not pathfinding. When all roads are equal, Dijkstra's doesn't need to evolve solutions — it simply explores outward in expanding circles, which is Breadth-First Search (BFS).",
      "1": "Exactly! When all road lengths are identical, Dijkstra's Algorithm has no reason to prefer one road over another, so it explores all neighboring nodes equally before moving further out. This creates an expanding circle pattern — exactly like BFS! Think of it as dropping a stone in still water: the ripples expand equally in all directions. Dijkstra's only behaves differently from BFS when edges have different weights (like varying traffic levels).",
      "2": "Not quite! Depth-First Search (DFS) goes as deep as possible down one path before backtracking — like following a maze by always turning right. Dijkstra's with equal weights doesn't do this; it explores all immediate neighbors first before moving further away. That expanding, layer-by-layer pattern is Breadth-First Search (BFS).",
      "3": "Not quite! A Random Walk picks directions randomly without any strategy, which is very inefficient. Dijkstra's Algorithm is systematic — with equal weights, it methodically explores all nodes at distance 1, then distance 2, then distance 3, and so on. This orderly expansion pattern is exactly Breadth-First Search (BFS)."
    }
  },
  {
    "id": 28,
    "question": "Your model takes 1 week to train. You want to speed it up. You have 2 computers. If you split the data in half and train on both simultaneously, then average the results, what is a potential issue?",
    "options": [
      "The computers might get heated up due to overclocking",
      "The accuracy will at least double due to parallel processing",
      "Need to constantly synchronize the 'learned knowledge' (weights) between them",
      "It is impossible as the computers are CPU and not GPU based"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! In distributed training, the two computers learn from different data subsets. Without frequent synchronization, their learned weights diverge — they develop different 'understandings.' Regularly sharing and averaging gradients or weights is essential but adds communication overhead.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Heat isn't the main issue here! While computers do generate heat, modern cooling systems handle this well. The real challenge in distributed training is keeping the two models synchronized. Since each computer sees different data, they learn different things. Without frequent communication to align their weights, the models diverge and the final result suffers.",
      "1": "Accuracy doesn't automatically double just because you use two computers! More hardware helps with speed, but splitting data means each model only sees half the picture. The key challenge is synchronization — the two models need to regularly share what they've learned (their gradients or weights) to stay aligned. Without this, they develop conflicting understandings of the data.",
      "2": "That's right! This is the core challenge of distributed training. When two computers train on different halves of the data, they develop different weights — like two students studying different chapters of the same textbook. To produce a good final model, they must frequently synchronize their learning (share gradients or weights). This communication adds overhead and is the main bottleneck in distributed AI training at scale.",
      "3": "CPU vs. GPU isn't the issue here! Distributed training works on both CPUs and GPUs. The real challenge is keeping the models synchronized. Whether you use CPUs or GPUs, splitting data across two machines means they learn different patterns. They need to regularly share their progress (weights/gradients) to converge on a good solution."
    }
  },
  {
    "id": 29,
    "question": "What is 'Early Stopping' in model training?",
    "options": [
      "Stopping after exactly x minutes of processing",
      "Stopping when the processor detects duplicate data",
      "Stopping the training when the error on the validation data stops improving",
      "Stopping the training well before the testing phase"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Early Stopping monitors the validation loss during training. When the validation loss stops improving (or starts increasing due to overfitting), training is halted and the best model is saved. It's a simple but powerful technique to prevent the model from memorizing training data.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "Not quite! Early Stopping isn't based on a fixed time limit. It monitors the validation performance and stops when improvement plateaus. A model might converge in 5 minutes or 5 hours depending on the task — the key is watching the validation loss, not the clock.",
      "1": "Not quite! Early Stopping has nothing to do with detecting duplicate data. It watches the validation loss (error on held-out data) during training. When the validation loss stops decreasing or starts going back up (a sign of overfitting), training is halted to preserve the best model found so far.",
      "2": "Exactly! Early Stopping is like knowing when to stop studying for a test. At some point, more studying (training) starts to hurt — you begin memorizing specific practice problems instead of understanding concepts (overfitting). Early Stopping detects this moment by monitoring performance on the validation set. When validation loss stops improving, it saves the best model and stops training. It's one of the simplest and most effective regularization techniques!",
      "3": "Not quite! Early Stopping doesn't mean stopping training before reaching the test phase — it means stopping the training process earlier than the maximum number of epochs. Specifically, it stops when the validation loss plateaus or increases, which signals overfitting. The 'early' refers to stopping before the model starts memorizing the training data."
    }
  },
  {
    "id": 30,
    "question": "You are designing a Face Recognition system for a school entrance. It works in the lab but fails when installed because the sunlight changes throughout the day. This is a failure of:",
    "options": [
      "Database size due to insufficient data captured of sunlight under different hours",
      "Algorithm speed due to inability to adapt to different amounts of sunlight",
      "Memory management issues related to changing temperature throughout the day",
      "Robustness to environmental conditions"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Robustness means a system's ability to work well under varying real-world conditions. The face recognition system was only tested in controlled lab lighting and couldn't handle the changing sunlight conditions of the real world. This gap between lab and real-world performance is a common challenge in deploying AI systems.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "While having more training data with different lighting conditions could help, the root issue is broader than just database size. The fundamental problem is robustness — the system wasn't designed or trained to handle varying environmental conditions. Even with more data, if the system architecture doesn't account for lighting variations, it may still fail.",
      "1": "Algorithm speed isn't the issue here! The system doesn't fail because it's too slow — it fails because it can't handle changing lighting conditions. A fast algorithm that gives wrong answers is still wrong. The problem is robustness: the system needs to produce correct results regardless of whether it's cloudy, sunny, dawn, or dusk.",
      "2": "Memory management isn't related to this problem! Changing temperatures don't significantly affect modern computer memory in normal operating conditions. The issue is that the face recognition model only learned to work under lab lighting conditions and can't handle the natural variation in sunlight throughout the day. This is a robustness problem.",
      "3": "That's right! Robustness is the ability to perform reliably under real-world conditions that may differ from the controlled environment where the system was developed and tested. Sunlight changes angle, intensity, and color throughout the day, creating shadows and glare that weren't present in the lab. This is a classic deployment challenge in AI — systems that work perfectly in the lab often struggle when faced with the messy, unpredictable real world. Solutions include data augmentation (training with varied lighting) and robust feature engineering."
    }
  },
  {
    "id": 31,
    "question": "You want to serve an AI model to millions of users. It takes 1 second to generate an answer. If 1000 users ask at the exact same second, what happens to a single server?",
    "options": [
      "It gets overwhelmed and crashes or makes users wait (Bottleneck)",
      "It speeds up by activating more cloud computing resources (Adaptability)",
      "It randomly responds to the first x selected users (Flexibility)",
      "It answers everyone instantly by scaling Moore's Law (Future-Oriented)"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A single server has a finite capacity (throughput). If 1000 users send requests simultaneously but the server can only handle a few per second, it creates a bottleneck — requests pile up, users experience long wait times, and the server might crash.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Exactly! This is a classic throughput bottleneck. Think of a single server like a single cashier at a store — if 1000 customers arrive at once but the cashier takes 1 second per customer, most people are stuck waiting in a very long queue. In AI systems, this means users see loading spinners, timeouts, or error messages. Solutions include adding more servers (horizontal scaling) and using load balancers to distribute traffic.",
      "1": "Nice thought, but a single server can't automatically summon more cloud resources on its own! Auto-scaling (automatically adding more servers when demand increases) is a solution that needs to be set up in advance. By default, a single server is like a single lane highway — if too many cars try to use it at once, traffic grinds to a halt.",
      "2": "Servers don't randomly select which users to serve! Without special configuration, most servers process requests in the order they arrive (like a queue). When overwhelmed, they either make users wait (increasing latency) or start rejecting requests (returning errors). This bottleneck is why scalable system design is so important in AI.",
      "3": "Moore's Law describes the historical trend of hardware getting faster over time, but it doesn't help in real-time! A server can't suddenly speed up because more users arrive. Its capacity is fixed at any given moment. When demand exceeds capacity, users wait. This is why scalable architectures (multiple servers, load balancers, caching) are essential for serving AI to millions of users."
    }
  },
  {
    "id": 32,
    "question": "You are collecting data for a medical AI. You have data from 5 hospitals. Hospital A uses 'Male/Female', Hospital B uses 'M/F', Hospital C uses '1/0'. Before training, what system step is crucial?",
    "options": [
      "Data Encryption",
      "Data Compression",
      "Data Adaptation",
      "Data Preparation"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Data Preparation (or Data Cleaning/Normalization) standardizes different formats into a consistent representation before training. The AI model can't learn effectively if 'Male', 'M', and '1' all mean the same thing but look different.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "Not quite! Data Encryption is about security — protecting data from unauthorized access (like locking a safe). It doesn't address the formatting inconsistency problem. Even after encrypting the data, the model still wouldn't know that 'Male', 'M', and '1' all represent the same concept. The data needs to be cleaned and standardized first.",
      "1": "Not quite! Data Compression reduces file size (like zipping a folder), but doesn't fix inconsistent formats. The AI model wouldn't know that 'Male', 'M', and '1' mean the same thing just because the data is compressed. You need Data Preparation to standardize the different representations into one consistent format.",
      "2": "While 'adaptation' sounds related, the standard term for this process is Data Preparation (or Data Cleaning/Normalization). This involves converting all the different representations ('Male/Female', 'M/F', '1/0') into a single consistent format so the AI model can learn from the combined data effectively.",
      "3": "Exactly! Data Preparation is one of the most important (and often most time-consuming!) steps in any AI project. Different hospitals using different formats ('Male/Female', 'M/F', '1/0') for the same information is a very common real-world problem. Before training, you must standardize everything — perhaps converting all entries to '0/1'. Without this step, the model might treat 'Male' and 'M' as completely different things. Data scientists often say they spend 80% of their time on data preparation!"
    }
  },
  {
    "id": 33,
    "question": "An AI system works on your powerful laptop but needs to run on a small smartwatch. What technique reduces the model size without losing too much accuracy?",
    "options": [
      "Using a smarter battery",
      "Overclocking processor",
      "Quantization (using fewer bits per number)",
      "Optimizing the code (using more efficient algorithms)"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Quantization converts high-precision numbers (like 32-bit floats) to lower precision (like 8-bit integers), dramatically shrinking the model's memory footprint and speeding up calculations with only minimal accuracy loss. It's a key technique for deploying AI on edge devices.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Battery capacity has nothing to do with model size! A smarter battery might let the device run longer, but it doesn't change how much memory the AI model requires or how fast it processes data. The solution is Quantization — representing the model's numbers with fewer bits, which directly reduces size and speeds up computation.",
      "1": "Overclocking (running the processor faster than its rated speed) can speed things up slightly, but it doesn't reduce the model size. A smartwatch has limited memory — if the model is too big to fit, running the processor faster won't help. Quantization directly addresses the size problem by using fewer bits per number (e.g., 8-bit instead of 32-bit), reducing size by up to 4x.",
      "2": "That's right! Quantization is like reducing the precision of measurements — instead of using 32-bit floating-point numbers (very precise, like measuring to the millionth of a millimeter), you use 8-bit integers (less precise, like measuring to the nearest millimeter). For most AI tasks, this small loss in precision barely affects accuracy, but it can reduce model size by 4x and significantly speed up computation. This is how AI runs on phones, watches, and other small devices!",
      "3": "While optimizing code can help with speed, it doesn't significantly reduce the model's memory footprint. The model's size comes from its weights — millions or billions of numbers stored in memory. Quantization directly tackles this by representing each number with fewer bits (8-bit instead of 32-bit), which is the most effective way to shrink a model for small devices."
    }
  },
  {
    "id": 34,
    "question": "A user reports that your AI gave a wrong answer. You want to fix it, but you don't want to break the answers it gets right. What system practice helps ensure this?",
    "options": [
      "Writing a more efficient AI from scratch",
      "Regression testing (Checking performance on a standard set of examples before every update)",
      "Improve the prompts (Using AI Agents)",
      "Adding a second AI with a more updated API access"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Regression testing uses a standard set of test cases that you run after every change to make sure the fix didn't break anything that was previously working. It's like a safety net that catches unintended side effects of code changes.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Writing from scratch is extremely risky and time-consuming! You'd lose all the correct behavior the current model has, and there's no guarantee the new version would be better. The smart approach is Regression Testing — keeping a standard set of test cases and running them after every change to ensure nothing previously working has been broken.",
      "1": "That's right! Regression testing is a fundamental software engineering practice used throughout the AI industry. You maintain a curated set of test examples (both old and new) and run them before every update. If the fix for the wrong answer also breaks previously correct answers, the tests catch it immediately. It's like a doctor checking that treating one symptom doesn't create new problems. This is especially important in AI, where changing one part of a model can have unexpected effects elsewhere.",
      "2": "While improving prompts can help in some cases, it doesn't provide the systematic safety guarantee that Regression Testing does. Prompt engineering might fix one case but could inadvertently break others. Regression testing catches these unintended consequences by checking all previous test cases after every change, ensuring nothing that worked before has been broken.",
      "3": "Adding a second AI doesn't solve the underlying problem and creates new complexity. You'd now have two models to maintain, and you still don't know if either one breaks existing functionality. Regression Testing is the proper engineering practice: maintain a test suite, run it after every change, and only deploy when all tests pass."
    }
  },
  {
    "id": 35,
    "question": "You have a dataset of 100 students. For each student, you record 3 grades: Math, Science, English. If you organize this into a matrix (grid of numbers) where rows are students and columns are grades, what are the dimensions of this matrix? Use the convention shape = rows x columns.",
    "options": [
      "100 x 3",
      "3 x 100",
      "103 x 1",
      "3 x 103"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! In matrix representation, rows typically represent samples (100 students) and columns represent features (3 grades). Following the convention: shape = rows × columns = 100 × 3. This is the standard layout used in data science and machine learning.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Exactly! In the standard data science convention, each row is one sample (student) and each column is one feature (grade subject). With 100 students and 3 grades, the matrix is 100 rows × 3 columns = 100 × 3. This layout is used everywhere in machine learning — libraries like Pandas, NumPy, and scikit-learn all expect data in this (samples × features) format.",
      "1": "The dimensions are swapped! 3 × 100 would mean 3 rows and 100 columns — that would represent 3 students with 100 grades each. The convention is rows = samples, columns = features. With 100 students (samples) and 3 grades (features), the correct shape is 100 × 3. Always remember: samples go in rows, features go in columns.",
      "2": "Not quite! 103 × 1 suggests adding 100 + 3 = 103, which doesn't make sense for matrix dimensions. Rows and columns are independent dimensions — you don't add them together. You have 100 students (rows) and 3 grades (columns), so the shape is 100 × 3. Think of it as a spreadsheet with 100 rows and 3 columns.",
      "3": "Not quite! 3 × 103 doesn't match any meaningful arrangement of this data. Matrix dimensions come directly from the structure: rows = number of samples (100 students), columns = number of features (3 grades). The correct shape is 100 × 3. Remember: rows and columns aren't added or otherwise combined — they're separate dimensions."
    }
  },
  {
    "id": 36,
    "question": "If we measure the height of 1000 randomly selected adults, most people will be around average height, with fewer very tall or very short people. What is the shape of this distribution curve often called?",
    "options": [
      "Linear Line (Uniform Distribution)",
      "Bell Curve (Normal Distribution)",
      "Random Scatter (Uniform Distribution)",
      "U-shape (Normal Distribution)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! When you measure a natural trait like height across many random people, most cluster around the average with fewer extremes. This creates the famous Bell Curve (Normal Distribution) — symmetric, with the peak at the mean. Many natural phenomena follow this pattern, and it's foundational to statistics and AI.",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "Not quite! A Linear Line / Uniform Distribution would mean every height is equally likely — a 150cm person would be just as common as a 180cm person, which doesn't match reality. In a uniform distribution, the graph would be flat (like a rectangle), not peaked in the middle. Height follows a Bell Curve because most people are near average height.",
      "1": "Exactly! This is the famous Normal Distribution, also called the Bell Curve because of its shape. Most people cluster near the average height (the peak of the bell), and there are progressively fewer people at extreme heights (the tails). This pattern appears everywhere in nature: test scores, measurement errors, biological traits, and even noise in AI sensors. Understanding the Normal Distribution is fundamental to statistics and machine learning!",
      "2": "Not quite! A Random Scatter / Uniform Distribution would show dots spread evenly everywhere with no clear pattern, meaning all heights are equally common. But that's not reality — most adults are near average height, with very few extremely tall or short people. The actual pattern is a Bell Curve (Normal Distribution) with a peak at the average.",
      "3": "Watch the label mismatch! A U-shape is not a Normal Distribution. A U-shaped curve would mean most people are either very short or very tall, with few people of average height — the opposite of reality! The Normal Distribution is bell-shaped (like an upside-down U), with most values clustered in the middle around the average."
    }
  },
  {
    "id": 37,
    "question": "In Python, if you have a list of AI confidence scores `scores = [0.1, 0.9, 0.4]` and you access `scores[1]`, what value do you get?",
    "options": [
      "Error",
      "0.9",
      "0.4",
      "0.1"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Python uses 0-based indexing, meaning the first element is at index 0, the second at index 1, and so on. So `scores[1]` returns the second element: 0.9.",
    "topic": "Programming",
    "subtopic": "Python",
    "feedback_map": {
      "0": "No error here! Index 1 is a valid position in this 3-element list (valid indices are 0, 1, and 2). You'd only get an error if you tried to access an index that doesn't exist, like `scores[3]` or `scores[5]`. The answer is 0.9 (the element at index 1).",
      "1": "That's right! Python uses 0-based indexing, which means counting starts at 0, not 1. So: `scores[0]` = 0.1, `scores[1]` = 0.9, `scores[2]` = 0.4. This is one of the first things to learn about Python (and most programming languages). Getting comfortable with 0-based indexing will save you many debugging headaches!",
      "2": "Not quite! 0.4 is at index 2, not index 1. Python counts from 0: `scores[0]` = 0.1, `scores[1]` = 0.9, `scores[2]` = 0.4. If you wanted 0.4, you'd use `scores[2]`. Remember: the index number is always one less than the position you might expect from everyday counting.",
      "3": "Close, but 0.1 is at index 0, not index 1! In Python, indexing starts at 0. So `scores[0]` = 0.1, `scores[1]` = 0.9. If you expected 0.1, you might be thinking of 1-based indexing (used in some other languages), but Python always starts counting from 0."
    }
  },
  {
    "id": 38,
    "question": "Before training an AI model, you want to quickly see the first 5 rows of your dataset `df` to check if it loaded correctly. Which command do you use?",
    "options": [
      "df.top()",
      "df.head()",
      "df.first()",
      "df.start()"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! `df.head()` is a Pandas function that returns the first 5 rows of a DataFrame by default. It's one of the most commonly used commands in data science — the first thing you do after loading any dataset!",
    "topic": "Programming",
    "subtopic": "Pandas",
    "feedback_map": {
      "0": "Close guess, but `top()` is not a standard Pandas method! The correct command is `df.head()`, which returns the first 5 rows by default. Many beginners try `top()` since it sounds logical, but Pandas chose `head()` (and `tail()` for the last rows). Think of it as 'head' = the beginning, like the head of a line.",
      "1": "That's right! `df.head()` is probably the most-used command in data science. It shows you the first 5 rows (by default) so you can quickly check that your data loaded correctly — right column names, right data types, no obvious errors. You can also specify a number like `df.head(10)` to see the first 10 rows. Its counterpart, `df.tail()`, shows the last rows.",
      "2": "Not quite! While `first()` sounds intuitive, it's not the standard Pandas method for viewing the first few rows. In Pandas, `first()` actually does something different (time-based selection). The correct command is `df.head()`, which returns the first 5 rows by default. It's one of the first Pandas commands every data scientist learns!",
      "3": "Not quite! `start()` is not a standard Pandas method. The correct command is `df.head()` — think of 'head' as the beginning of the DataFrame. It returns the first 5 rows by default. Similarly, `df.tail()` shows the last rows. These are essential tools for quickly inspecting your data."
    }
  },
  {
    "id": 39,
    "question": "You get an error: `RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)`. Matrix multiplication requires the inner dimensions to match (e.g., 2x3 and 3x4). Why did this fail?",
    "options": [
      "The matrices need to be entered in the correct sequence",
      "The computer can only multiply matrices containing floating point numbers",
      "The computer is not programmed to handle small matrix dimensions as it is trained on big data",
      "The columns of the first matrix (3) do not match the rows of the second matrix (2)"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Matrix multiplication requires the number of columns in the first matrix to equal the number of rows in the second matrix. For 2×3 and 2×3, the inner dimensions are 3 and 2 — they don't match (3 ≠ 2). You'd need to transpose the second matrix to 3×2 for the multiplication to work.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "While matrix order does matter in multiplication (A×B ≠ B×A in general), that's not the core issue here. The problem is the dimension mismatch: a 2×3 matrix has 3 columns, but the second 2×3 matrix has 2 rows. For multiplication, the columns of the first must equal the rows of the second (3 ≠ 2). Swapping the order (making it 2×3 times 2×3 the other way) still has the same mismatch.",
      "1": "That's not the issue! Matrix multiplication works with any numeric types (integers, floats, etc.). The error is about shapes: for A×B to work, if A is (m×n), then B must be (n×p) — the inner dimensions must match. Here, the first matrix has 3 columns and the second has 2 rows. Since 3 ≠ 2, the multiplication fails.",
      "2": "Matrix size has nothing to do with 'big data' requirements! The error occurs because of a shape mismatch. For matrix multiplication A×B, the number of columns in A must equal the number of rows in B. Here, both are 2×3: A has 3 columns but B has only 2 rows. Since 3 ≠ 2, the operation fails. You'd need to transpose B to make it 3×2.",
      "3": "Exactly! Matrix multiplication has a strict rule: if A is (m×n) and B is (r×p), then n must equal r — the inner dimensions must match. Here, both matrices are 2×3. The first has 3 columns (n=3) and the second has 2 rows (r=2). Since 3 ≠ 2, the multiplication fails. The fix is to transpose the second matrix from 2×3 to 3×2. Then it becomes (2×3) × (3×2) = (2×2), which works perfectly!"
    }
  },
  {
    "id": 40,
    "question": "You have a dataset of 2 points: A(1, 1) and B(5, 5). You use a K-Nearest Neighbor (KNN) classifier with K=1. If a new point appears at (2, 2), which point is its nearest neighbor?",
    "options": [
      "Both are equal distance",
      "Neither",
      "A(1, 1)",
      "B(5, 5)"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! The distance from (2,2) to A(1,1) is √2 ≈ 1.41, while the distance to B(5,5) is √18 ≈ 4.24. Since A is much closer, KNN with K=1 classifies the new point based on A's label. This is the core idea of KNN — 'you are who your neighbors are.'",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "They're not equal! Let's calculate: Distance to A = √((2-1)² + (2-1)²) = √(1+1) = √2 ≈ 1.41. Distance to B = √((5-2)² + (5-2)²) = √(9+9) = √18 ≈ 4.24. A is about 3 times closer! Point (2,2) is much nearer to A(1,1) than to B(5,5).",
      "1": "With K=1, there must be a nearest neighbor — 'Neither' isn't possible. KNN always finds the closest point(s). Here, A(1,1) is at distance √2 ≈ 1.41 and B(5,5) is at distance √18 ≈ 4.24. A is clearly the nearest neighbor, so the new point takes A's classification.",
      "2": "That's right! Using the Euclidean distance formula: Distance to A = √((2-1)² + (2-1)²) = √2 ≈ 1.41, and Distance to B = √((5-2)² + (5-2)²) = √18 ≈ 4.24. A is about 3 times closer! With K=1 (looking at just the 1 nearest neighbor), the new point at (2,2) is classified the same as A. This is the heart of KNN: classify new points based on their closest training examples.",
      "3": "Not this one! B(5,5) is much farther away. Distance to B = √((5-2)² + (5-2)²) = √18 ≈ 4.24, while distance to A = √((2-1)² + (2-1)²) = √2 ≈ 1.41. Point (2,2) is about 3 times closer to A(1,1) than to B(5,5), so A is the nearest neighbor."
    }
  },
  {
    "id": 41,
    "question": "A photo filter 'flips' an image horizontally. If a point on the right side of the image is at [5, 2], and the center is 0, flipping it moves it to [-5, 2]. Which operation describes this 'flip'?",
    "options": [
      "Multiply the first number (x-coordinate) by -1",
      "Multiply both by -1",
      "Multiply the second number (y-coordinate) by -1",
      "Swap the two numbers"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A horizontal flip across the y-axis negates the x-coordinate while keeping y the same: (x, y) → (-x, y). So [5, 2] becomes [-5, 2]. This is a fundamental transformation used in image processing and data augmentation for AI.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Spot on! Flipping horizontally means reflecting across the y-axis, which only affects the x-coordinate: (x, y) → (-x, y). The y-coordinate stays the same because the point doesn't move up or down. This transformation is widely used in AI for data augmentation — flipping training images horizontally to create more varied training examples.",
      "1": "Not quite! Multiplying both coordinates by -1 (getting [-5, -2]) would rotate the point 180° around the origin, not flip it horizontally. A horizontal flip only changes the left-right position (x-coordinate), not the up-down position (y-coordinate). The correct operation is (-x, y) = (-5, 2).",
      "2": "That would be a vertical flip! Negating the y-coordinate (getting [5, -2]) flips the point up and down, not left and right. For a horizontal flip (left-right mirror), you negate the x-coordinate: (x, y) → (-x, y) = (-5, 2). Remember: horizontal flip changes x, vertical flip changes y.",
      "3": "Swapping the coordinates (getting [2, 5]) would reflect the point across the diagonal line y = x. That's neither a horizontal nor a vertical flip. For a horizontal flip, you negate the x-coordinate: (x, y) → (-x, y) = (-5, 2). Each type of transformation has its own specific mathematical operation."
    }
  },
  {
    "id": 42,
    "question": "An AI model is like a function machine f(x). You feed it an image x, and it outputs a label y. If the machine always predicts 'Cat' regardless of the image, which equation best represents this behavior?",
    "options": [
      "x = 'Cat'",
      "f(x) = 'Cat'",
      "f(x) = x",
      "f('Cat') = x"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! f(x) = 'Cat' means the function always returns the constant value 'Cat' regardless of what input x is. This represents a model that hasn't learned anything useful — it just predicts the same class for everything.",
    "topic": "Mathematics",
    "subtopic": "Calculus",
    "feedback_map": {
      "0": "x = 'Cat' is a variable assignment, not a function definition. It just sets the value of x to 'Cat' — it doesn't describe the behavior of a model taking input and producing output. The model's behavior is a function: f(x) = 'Cat' means no matter what image x you feed in, the output is always 'Cat'.",
      "1": "Exactly! f(x) = 'Cat' perfectly captures this behavior: the function f takes any input x (any image) and always returns the same output — 'Cat'. This is called a constant function. In AI, a model that always predicts the same class regardless of input hasn't learned anything useful. It might happen when there's a bug in training or when one class dominates the dataset.",
      "2": "Not quite! f(x) = x means the function returns the input itself — so if you feed in an image, it would output that same image (an identity function). That's very different from always predicting 'Cat'. The correct representation is f(x) = 'Cat', which means the output is always the constant 'Cat' regardless of input.",
      "3": "This is backwards! f('Cat') = x would mean: when you input the word 'Cat,' the function returns some value x. But the model's behavior is the opposite — you input any image x and always get back 'Cat'. The correct notation is f(x) = 'Cat': any input → always 'Cat'."
    }
  },
  {
    "id": 43,
    "question": "An AI is trained to classify images as 'Dog' or 'Not Dog'. If the AI randomly guesses (50/50 chance), what is the probability it guesses correctly 3 times in a row?",
    "options": [
      "1/8 (12.5%)",
      "1/4 (25%)",
      "1/3 (33.3%)",
      "1/2 (50%)"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! For independent events, you multiply the individual probabilities. With a 50% (1/2) chance each time: (1/2) × (1/2) × (1/2) = 1/8 = 12.5%. This shows that even random guessing can occasionally appear successful over short streaks!",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "That's right! For independent events, the probability of all events happening together is the product of individual probabilities: (1/2) × (1/2) × (1/2) = 1/8 = 12.5%. This means even a random-guessing AI will get 3 in a row correct about 12.5% of the time! This is important in AI: short winning streaks don't prove a model is good — you need to test over many examples to know its true accuracy.",
      "1": "Close, but 1/4 (25%) is the probability of getting 2 correct in a row, not 3. For 2 guesses: (1/2) × (1/2) = 1/4. For 3 guesses, you need one more multiplication: (1/2)³ = 1/8 = 12.5%. Each additional correct guess halves the probability!",
      "2": "Not quite! 1/3 isn't how probability multiplication works. You might be thinking of '1 divided by the number of events,' but that's not the right formula. For independent events, multiply the probabilities: (1/2) × (1/2) × (1/2) = 1/8 = 12.5%. The probability shrinks by half with each additional event.",
      "3": "1/2 (50%) is the probability of getting just one guess correct. For 3 correct guesses in a row, you need to multiply: (1/2) × (1/2) × (1/2) = 1/8. Each additional correct guess makes the overall probability smaller. Think of it as stacking requirements: correct AND correct AND correct."
    }
  },
  {
    "id": 44,
    "question": "An image in AI is often represented as a NumPy array of shape `(100, 100, 3)`. What do these numbers likely represent?",
    "options": [
      "Length, Width, Height",
      "Quality, Size, Format",
      "Height 100, Width 100, 3 Color Channels (RGB)",
      "100 images, 100 pixels, 3 colors"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! The shape (100, 100, 3) means: 100 pixels tall (height), 100 pixels wide (width), and 3 color channels (Red, Green, Blue). This is the standard way images are represented as numerical arrays in AI and computer vision.",
    "topic": "Programming",
    "subtopic": "NumPy",
    "feedback_map": {
      "0": "Not quite! Images are 2D surfaces (not 3D objects), so Length/Width/Height doesn't apply. The three numbers represent the image's pixel height (100), pixel width (100), and color channels (3 for RGB). Think of it as a stack of three 100×100 grids — one for Red, one for Green, one for Blue.",
      "1": "These numbers represent physical dimensions, not abstract qualities. Quality/Size/Format would be metadata about the file, not the array structure. The shape (100, 100, 3) directly tells you: the image is 100 pixels tall, 100 pixels wide, with 3 color values (RGB) per pixel.",
      "2": "Exactly! (100, 100, 3) is the standard image representation in AI: Height × Width × Channels. Each pixel has 3 values — one each for Red, Green, and Blue — typically ranging from 0 to 255. A 100×100 image with 3 channels means the total array contains 100 × 100 × 3 = 30,000 numbers. Understanding image shapes is essential for building computer vision models!",
      "3": "Not quite! This represents a single image, not 100 images. If you had a batch of 100 images, the shape would be (100, H, W, 3), with an extra dimension for the batch. Here, (100, 100, 3) means one image that's 100 pixels tall, 100 pixels wide, with 3 color channels (RGB)."
    }
  },
  {
    "id": 45,
    "question": "You want to filter a DataFrame to show only students with a grade above 50. Which syntax looks most correct?",
    "options": [
      "df.filter(50)",
      "df['grade' > 50]",
      "if df['grade'] > 50:",
      "df[df['grade'] > 50]"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! `df[df['grade'] > 50]` uses Boolean Indexing: the inner expression creates a True/False mask for each row, and the outer brackets select only the rows where the condition is True.",
    "topic": "Programming",
    "subtopic": "Pandas",
    "feedback_map": {
      "0": "Not a standard Pandas command! `df.filter()` exists in Pandas but works on column/row labels, not on data values. For filtering rows by a condition, you use Boolean Indexing: `df[df['grade'] > 50]`. The inner `df['grade'] > 50` creates a True/False list, and the outer `df[...]` keeps only the True rows.",
      "1": "Invalid syntax! `df['grade' > 50]` would evaluate 'grade' > 50 first (comparing a string to a number, which doesn't make sense). The correct syntax is `df[df['grade'] > 50]` — note the `df` inside the brackets. You need `df['grade']` to access the column, then `> 50` to create the condition.",
      "2": "Python's `if` statement doesn't work on entire DataFrame columns! It expects a single True/False value, but `df['grade'] > 50` produces a True/False value for every row. Instead, use Boolean Indexing: `df[df['grade'] > 50]`, which applies the condition to all rows simultaneously and returns only the matching ones.",
      "3": "That's right! This is Boolean Indexing — a powerful Pandas technique. Step by step: (1) `df['grade']` selects the grade column, (2) `> 50` compares each value to 50, creating a True/False series, (3) `df[...]` uses this series to filter rows, keeping only where the condition is True. This pattern is used constantly in data science!"
    }
  },
  {
    "id": 46,
    "question": "You are building a spam filter. It is very dangerous to mark a real email as spam (False Positive), but okay to let a few spam emails through (False Negative). Which model should you choose?",
    "options": [
      "95% Accuracy, but marks 0% of real emails as spam",
      "99% Accuracy, but marks 5% of real emails as spam",
      "100% Accuracy on training data only",
      "50% Accuracy on training data only"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! When False Positives are very costly (losing important real emails), you should prioritize minimizing them, even if it means letting some spam through. A 95% accurate model with 0% False Positives is safer than a 99% accurate model that occasionally deletes real emails.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "Exactly! In this scenario, a False Positive (marking a real email as spam) could mean losing a job offer, an important deadline, or a medical result. The 95% accuracy model with 0% False Positive rate is the safest choice — it catches most spam while never accidentally deleting a real email. This illustrates a key AI concept: accuracy alone isn't enough; you must consider which types of errors matter most for your specific use case.",
      "1": "Think carefully about the cost of errors! While 99% sounds better than 95%, this model marks 5% of real emails as spam. That means 5 out of every 100 legitimate emails could be silently deleted — including important ones! In email filtering, losing real messages is far more damaging than letting some spam through. The 95% model with 0% False Positives is the safer choice.",
      "2": "Training accuracy alone is misleading! A model that's 100% accurate on training data but hasn't been tested on new data might be severely overfitting. More importantly, this option doesn't tell us anything about False Positive rates on real-world data. We need a model that performs well on new, unseen emails — especially one that doesn't accidentally delete real messages.",
      "3": "50% accuracy is essentially random guessing — flipping a coin for every email! This would be useless as a spam filter. Plus, 'on training data only' suggests it hasn't been evaluated on real-world data. We need a model with high accuracy on new emails AND, critically, a very low False Positive rate so real emails are never lost."
    }
  },
  {
    "id": 47,
    "question": "You have a vertical edge detection filter (kernel). If you slide it over a horizontal line in an image, what will the output likely be?",
    "options": [
      "A strong bright line, because the patterns match",
      "Near zero (black), because the patterns don't match",
      "A vertical line",
      "A diagonal line"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! A convolution filter activates when the input pattern matches its pattern. A vertical edge detector looks for changes in the horizontal direction (left-right brightness changes). A horizontal line has no such change — it's constant horizontally — so the filter produces near-zero output.",
    "topic": "AI-ML-DL",
    "subtopic": "Computer Vision",
    "feedback_map": {
      "0": "Not quite — the patterns don't match! A vertical edge detection filter detects vertical edges (left-to-right changes in brightness). A horizontal line changes in the vertical direction but is constant horizontally. Since the filter is looking for horizontal changes and there are none, the response is near zero. The filter would light up for a vertical line, not a horizontal one.",
      "1": "That's right! Convolution filters work by pattern matching — they 'activate' when the input pattern aligns with the filter pattern. A vertical edge filter detects left-to-right brightness changes. A horizontal line is uniform from left to right (no horizontal change), so the filter response is near zero (black). This is a fundamental concept in CNNs: different filters detect different features, and a mismatch produces no response.",
      "2": "Not quite! The output wouldn't be a vertical line. A vertical edge filter on a horizontal line produces near-zero output everywhere because the patterns don't match. You'd get a vertical line in the output only if the input contained vertical edges. The convolution filter responds to patterns similar to its own shape.",
      "3": "Not quite! Diagonal outputs would come from applying a diagonal edge filter. A vertical filter on a horizontal line produces near-zero output because the filter's pattern doesn't match what's in the image. Convolution is about pattern matching: vertical filter → detects vertical features, horizontal filter → detects horizontal features."
    }
  },
  {
    "id": 48,
    "question": "A diffusion model generates images by adding and then removing noise. If you stop the process halfway through, what will the image look like?",
    "options": [
      "A blurry grainy image",
      "A perfect image",
      "A black screen",
      "Pure white noise"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Diffusion models start with pure random noise and gradually refine it into a clear image. Halfway through, the image is partially formed — you can see emerging shapes and structure, but it's still noticeably noisy and grainy. It's like watching a photo slowly develop.",
    "topic": "AI-ML-DL",
    "subtopic": "Generative AI",
    "feedback_map": {
      "0": "Exactly! A diffusion model works like a photo slowly coming into focus. It starts with pure noise (TV static) and removes a bit of noise at each step. Halfway through, you'd see blurry, grainy shapes emerging from the noise — recognizable outlines but with significant graininess still present. This gradual refinement process is how tools like DALL-E and Stable Diffusion create images.",
      "1": "Not yet! A perfect image is what you get at the very end of the process, after all the noise has been removed. Halfway through, the denoising isn't complete — the image has recognizable shapes but is still covered in significant noise and grain. The longer the process runs, the cleaner the image becomes.",
      "2": "A black screen isn't what you'd see! The image starts as random colored noise (like TV static), not blackness. Halfway through the denoising process, that noise has partially organized into blurry shapes and structures, but there's still significant graininess. It would look like a very noisy, partially-formed version of the final image.",
      "3": "Pure white noise is what the image looks like at the very beginning, before any denoising has occurred. Halfway through, the model has already removed a lot of that noise, revealing emerging image structure. The result is a grainy, blurry image that's somewhere between random noise and a clear picture."
    }
  },
  {
    "id": 49,
    "question": "You are training a model. You can process 1 image at a time (Batch Size = 1) or 100 images at a time (Batch Size = 100). Why is Batch Size = 100 usually faster on modern hardware?",
    "options": [
      "It is not faster but marketing gimmick",
      "Because GPUs can calculate the math for many images in parallel simultaneously",
      "Because the images are smaller so requires less storage and hence faster processing",
      "Because it can intelligently skips similar images"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! GPUs (Graphics Processing Units) are designed for parallel computation. They can perform the same mathematical operation on hundreds or thousands of data points simultaneously, making batch processing dramatically faster than one-at-a-time processing.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "It genuinely is faster, not a gimmick! GPUs can process many images in parallel, meaning a batch of 100 images takes only slightly longer than a single image. This massive speedup is why GPUs revolutionized AI training. Without parallel batch processing, training modern AI models would take months instead of days.",
      "1": "That's right! GPUs contain thousands of small cores designed to do the same math operation on many pieces of data simultaneously (called SIMD — Single Instruction, Multiple Data). This means multiplying a weight matrix by 100 image vectors takes almost the same time as multiplying by just 1. It's like having 100 cashiers instead of 1 — 100 customers are served almost as fast as 1. This parallelism is the main reason AI training uses GPUs!",
      "2": "Images aren't made smaller by batching! Each image keeps its full size and quality. The speedup comes from GPU parallelism — the GPU processes all 100 images through the same mathematical operations at the same time. It's like 100 students taking a test simultaneously vs. one at a time — the test isn't shorter, but all students finish together.",
      "3": "The GPU doesn't skip any images! Every image in the batch is fully processed with exactly the same computations. The speed advantage comes from parallelism — the GPU handles all 100 images simultaneously rather than sequentially. Think of it as a copy machine that can print 100 copies at once, rather than one at a time."
    }
  },
  {
    "id": 50,
    "question": "Imagine a robot on a 2D grid at position (0,0). A command 'v' moves it 3 units right and 2 units up. If we represent this command as a vector v = [3, 2], what would be the position of the robot after applying the command twice? Assume coordinates are (x, y) where x increases to the right and y increases upward. In machine learning, this kind of computation commonly appears when working with feature vectors, embeddings, or gradients.",
    "options": [
      "(6, 4)",
      "(5, 4)",
      "(9, 4)",
      "(3, 2)"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Applying a vector twice means adding it twice: [3,2] + [3,2] = [6,4], or equivalently, 2 × [3,2] = [6,4]. Scalar multiplication multiplies each component separately: 2×3=6, 2×2=4.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "That's right! Starting at (0,0), applying v=[3,2] twice means: first move → (0+3, 0+2) = (3,2), second move → (3+3, 2+2) = (6,4). This is the same as scalar multiplication: 2 × [3,2] = [2×3, 2×2] = [6,4]. In AI, this same math applies when scaling gradients, adjusting feature vectors, or computing embeddings!",
      "1": "Not quite! (5,4) doesn't come from a consistent operation on [3,2]. You might have added 2 to x but correctly doubled y? Remember, applying a vector twice means adding it to itself: [3,2] + [3,2] = [3+3, 2+2] = [6,4]. Both components are treated the same way.",
      "2": "Not quite! (9,4) might come from cubing x (3³=27... no) or multiplying x by 3 (3×3=9) while only doubling y. The correct operation is consistent: multiply both components by 2 (since we apply the vector twice). 2 × [3,2] = [6,4]. Both components should be multiplied by the same scalar.",
      "3": "(3,2) is the position after applying the vector just once! The question asks about applying it twice. After the first application: (0+3, 0+2) = (3,2). After the second: (3+3, 2+2) = (6,4). Two applications means adding the vector twice."
    }
  },
  {
    "id": 51,
    "question": "You are pushing a box with a force vector A = [5, 0] (pushing right). Your friend pushes the same box with force vector B = [0, 5] (pushing up). If both push at the same time, in which direction does the box move?",
    "options": [
      "It moves diagonally up-right",
      "It does not move beyond fixed bounds limits",
      "It moves right then up",
      "It moves diagonally down-left"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! When two forces act simultaneously, the resultant is their vector sum: [5,0] + [0,5] = [5,5]. This vector points diagonally up and to the right at a 45° angle. Vector addition is how forces, velocities, and AI gradients combine!",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "That's right! Forces applied simultaneously combine through vector addition: [5,0] + [0,5] = [5,5]. This resultant vector points at 45° up-right because both components are equal. This principle is fundamental in physics and AI — gradient vectors from different parameters combine the same way to determine the direction of model updates!",
      "1": "The box does move! Two forces acting together produce a combined (resultant) force. [5,0] (right) + [0,5] (up) = [5,5] (diagonal up-right). There are no 'bounds limits' in this scenario. Whenever multiple forces act on an object, they add together to produce motion.",
      "2": "Not quite! The box doesn't move right first, then up — both forces act simultaneously. The combined effect is a single diagonal movement. Vector addition: [5,0] + [0,5] = [5,5], which points diagonally up-right. Think of it like a river current (pushing right) while you swim upstream (pushing up) — you'd move diagonally, not in two separate steps.",
      "3": "Opposite direction! Down-left would be [-5,-5], which would require forces pushing left and down. The forces here push right [5,0] and up [0,5], so the resultant is [5,5] — diagonally up and to the right at 45°."
    }
  },
  {
    "id": 52,
    "question": "An image is represented by a grid of pixels. If we represent the location of a pixel as a vector x, and we want to zoom in by 2x, we multiply the vector by 2. If a pixel was at [10, 20], where is it now?",
    "options": [
      "[12, 22]",
      "[40, 20]",
      "[20, 40]",
      "[10, 20]"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Scalar multiplication multiplies each component by the scalar: 2 × [10,20] = [2×10, 2×20] = [20,40]. This is how zooming works — every coordinate is scaled by the same factor, expanding the image uniformly.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "[12,22] comes from adding 2, not multiplying by 2. Zooming by 2x means every coordinate doubles: 2 × [10,20] = [20,40]. Adding 2 would shift the pixel's position, not scale the image. Multiplication scales, addition translates.",
      "1": "[40,20] has the components in the wrong order or applied inconsistently. Scalar multiplication applies equally to all components: 2 × [10,20] = [2×10, 2×20] = [20,40]. Both x and y are multiplied by 2.",
      "2": "Exactly! Scalar multiplication is beautifully simple: multiply each component by the scalar. 2 × [10,20] = [20,40]. In image processing, this is how zooming works — every pixel's position is scaled by the same factor, making the image twice as large. This same operation appears throughout AI: scaling weight vectors, adjusting learning rates, normalizing features.",
      "3": "[10,20] is the original position — nothing has changed! To zoom by 2x, you multiply each coordinate by 2: 2 × [10,20] = [20,40]. Keeping the position at [10,20] means no zoom was applied."
    }
  },
  {
    "id": 53,
    "question": "User A likes Action (score 5) and Comedy (score 1). User B likes Action (score 4) and Comedy (score 5). We can represent them as vectors A=[5, 1] and B=[4, 5]. The 'similarity' score is calculated by multiplying matching categories and summing them up. What is the similarity score?",
    "options": [
      "29",
      "15",
      "25",
      "20"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! This is the Dot Product: multiply corresponding elements and add them up. (5×4) + (1×5) = 20 + 5 = 25. The dot product measures how similar two vectors are — higher values mean more similar preferences. It's used everywhere in AI recommendation systems!",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Not quite! 29 might come from a calculation error like (5×4) + (1+5) = 20 + 6... that still doesn't give 29. The correct dot product multiplies matching categories and sums: (5×4) + (1×5) = 20 + 5 = 25. Make sure you multiply (not add) corresponding elements.",
      "1": "Not quite! Check the arithmetic: the dot product is (5×4) + (1×5) = 20 + 5 = 25, not 15. You might have calculated one of the products incorrectly. Remember: multiply matching elements, then sum all the products.",
      "2": "That's right! The Dot Product is: (5×4) + (1×5) = 20 + 5 = 25. This is one of the most important operations in AI! In recommendation systems like Netflix or Spotify, user preferences are stored as vectors, and the dot product measures similarity. A higher score means more similar tastes. Here, both users like Action, which contributes the most (5×4=20) to their similarity score.",
      "3": "Not quite! 20 is just the first product (5×4) — you forgot to add the second product (1×5=5). The full dot product sums all matching-category products: (5×4) + (1×5) = 20 + 5 = 25. Don't stop before adding all the terms!"
    }
  },
  {
    "id": 54,
    "question": "Shop A sells Apples and Bananas. In January, sales matrix is [100, 50]. In February, sales matrix is [120, 60]. To find the total sales for both months, what is the resulting matrix?",
    "options": [
      "[220, 110]",
      "[220, 60]",
      "[120, 110]",
      "[120, 50]"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Matrix (vector) addition adds corresponding elements: [100+120, 50+60] = [220, 110]. January apples plus February apples = total apples, and the same for bananas. This element-wise addition is one of the most basic and important operations in data processing.",
    "topic": "Mathematics",
    "subtopic": "Linear Algebra",
    "feedback_map": {
      "0": "Exactly! Matrix addition adds each element to its matching counterpart: Apples: 100 + 120 = 220, Bananas: 50 + 60 = 110. Result: [220, 110]. This element-wise operation keeps categories aligned — apple sales add with apple sales, banana sales add with banana sales. Simple but fundamental!",
      "1": "Not quite! [220, 60] correctly adds the apples (100+120=220) but uses only February's banana count (60). You need to add both months' bananas too: 50+60=110. The correct answer is [220, 110]. Make sure you add both corresponding elements.",
      "2": "Not quite! [120, 110] correctly adds the bananas (50+60=110) but uses only February's apple count (120). You need to add both months' apples too: 100+120=220. The correct answer is [220, 110]. Each element must be summed with its matching counterpart.",
      "3": "[120, 50] just mixes February apples with January bananas — no addition was performed at all! To find total sales, add corresponding elements: Apples: 100+120=220, Bananas: 50+60=110. Result: [220, 110]."
    }
  },
  {
    "id": 55,
    "question": "A self-driving car's position changes over time. If the car travels 100 meters in 5 seconds at a constant speed, what is its rate of change of position (speed)?",
    "options": [
      "20 m/s",
      "0.05 m/s",
      "500 m/s",
      "95 m/s"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Rate of change (speed) = Distance / Time = 100m / 5s = 20 m/s. In calculus terms, this is the derivative of position with respect to time — a concept that's central to how AI models learn and update their parameters.",
    "topic": "Mathematics",
    "subtopic": "Calculus",
    "feedback_map": {
      "0": "That's right! Speed = Distance ÷ Time = 100m ÷ 5s = 20 m/s. This 'rate of change' concept is everywhere in AI: the gradient (rate of change of error with respect to weights) tells the model how to adjust its parameters. Just as speed tells us how fast position changes, gradients tell us how fast the error changes when we tweak the model.",
      "1": "You divided in the wrong direction! 0.05 = 5/100 (time divided by distance). Speed is distance divided by time: 100m ÷ 5s = 20 m/s. An easy way to remember: speed asks 'how much distance per unit of time?'",
      "2": "Not quite! 500 comes from multiplying (100 × 5), not dividing. Speed is a rate — distance divided by time: 100m ÷ 5s = 20 m/s. Multiplication would give area or work, depending on context, not speed.",
      "3": "Not quite! 95 might come from subtracting (100 - 5), but speed involves division, not subtraction. Speed = Distance ÷ Time = 100m ÷ 5s = 20 m/s. Remember: rate of change always involves division — how much one quantity changes per unit of another."
    }
  },
  {
    "id": 56,
    "question": "A robot's speed varies over time. If you plot Speed vs. Time on a graph, what does the area under the line represent?",
    "options": [
      "Average speed",
      "Maximum speed",
      "Total distance traveled",
      "Current acceleration"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! The area under a Speed vs. Time graph represents the total distance traveled. This is the concept of integration in calculus: (m/s) × (s) = meters. This principle is widely used in AI for computing accumulated values and probabilities.",
    "topic": "Mathematics",
    "subtopic": "Calculus",
    "feedback_map": {
      "0": "Not quite! Average speed would be represented by a single horizontal line, not the area under the curve. The area under the Speed vs. Time graph represents total distance traveled. Mathematically: speed × time = distance, and the area calculation extends this to varying speeds.",
      "1": "Maximum speed would be the highest point on the graph (the peak), not the area underneath it. The area under the Speed vs. Time curve represents total distance traveled. Think of it this way: if you drive at 60 km/h for 2 hours, the area is 60 × 2 = 120 km — the total distance!",
      "2": "Exactly! The area under a Speed vs. Time graph gives the total distance traveled. This is the fundamental concept of integration in calculus. Units confirm this: (meters/second) × (seconds) = meters. For constant speed, it's a simple rectangle. For varying speed, the area captures the accumulated distance. This same principle is used in AI for computing accumulated losses, probabilities under curves, and many other applications.",
      "3": "Acceleration is the slope (steepness) of the Speed vs. Time graph, not the area under it. Slope tells you how fast speed is changing, while area tells you total distance traveled. Remember: slope = rate of change (acceleration), area = accumulated total (distance)."
    }
  },
  {
    "id": 57,
    "question": "A disease is rare (1% of people have it). A test is 99% accurate. If you test positive, is the probability you have the disease definitely 99%?",
    "options": [
      "No, it is 0%",
      "No, it is likely much lower because the disease is rare",
      "Yes, exactly 99%",
      "Yes, roughly 99%"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! This is the Base Rate Fallacy (related to Bayes' Theorem). Even with a 99% accurate test, when a disease is very rare (1%), most positive results are actually false positives. Out of ~20 positive tests from 1000 people, only about 10 truly have the disease — roughly 50%, not 99%!",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "The probability isn't 0%! You do have some chance of having the disease after testing positive. But it's also not 99%. Because the disease is rare, many positive results come from the 990 healthy people (even the 1% error rate creates ~10 false positives). The actual probability is around 50%, not 0%.",
      "1": "That's right! This is one of the most important and counterintuitive ideas in probability — the Base Rate Fallacy. Here's why: out of 1000 people, about 10 have the disease and 990 don't. The test correctly identifies ~9.9 of the 10 sick people. But it also falsely flags ~9.9 of the 990 healthy people. So out of ~20 positive results, only about 10 are real — that's roughly 50%, not 99%! This concept is crucial in AI, medical testing, and security screening.",
      "2": "It seems intuitive, but it's wrong! This is the famous Base Rate Fallacy. The 99% accuracy applies to the test itself, but when the disease is very rare (1%), most of the population is healthy. Even a small error rate (1%) applied to 990 healthy people produces about 10 false positives — roughly as many as the true positives from the 10 sick people. The actual probability is around 50%.",
      "3": "Close to 99% sounds right but isn't! The test accuracy and the probability you have the disease are different things. Because 99% of people don't have the disease, the small error rate produces many false positives among the large healthy population. Working through the math (Bayes' Theorem), the probability is actually around 50%. This unintuitive result is why base rates matter so much!"
    }
  },
  {
    "id": 58,
    "question": "You think your friend is at the park (50% sure). You call them, and hear birds chirping in the background (which is common at the park, rare at home). How should your belief change?",
    "options": [
      "You should be less sure",
      "You should be more than 50% sure",
      "You should be 100% sure",
      "You should stay 50% sure"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! This is Bayesian updating — you update your beliefs based on new evidence. Hearing birds (more likely at a park than at home) increases your confidence that your friend is at the park. The probability goes above 50% but not to 100%, since birds could occasionally be heard at home too.",
    "topic": "Mathematics",
    "subtopic": "Probability",
    "feedback_map": {
      "0": "The opposite! Hearing birds should make you more sure, not less. Birds chirping is more common at a park than at home, so this evidence supports the 'at the park' hypothesis. In Bayesian reasoning, evidence that's more consistent with one hypothesis strengthens your belief in it.",
      "1": "Exactly! This is Bayesian reasoning in action — updating beliefs with new evidence. Before the call, you were 50/50. The birds chirping is more likely at a park than at home, so this evidence shifts the probability toward 'at the park' — maybe 70-80%, depending on how rare birds are at home. This is exactly how AI systems update their predictions with new data!",
      "2": "Almost, but 100% is too strong! While birds support the park hypothesis, there's still some chance your friend is at home (maybe they have a bird feeder, or a window is open). In Bayesian reasoning, we rarely reach 100% certainty — we update our probability in the direction the evidence points, but leave room for alternative explanations.",
      "3": "The evidence matters! Hearing birds should change your belief because birds are more common at parks than at homes. Staying at 50% would mean ignoring relevant evidence. In Bayesian reasoning, new information that favors one hypothesis over another should shift your probability accordingly."
    }
  },
  {
    "id": 59,
    "question": "A student memorizes all the answers to the practice test but doesn't understand the concepts. They get 100% on the practice test but fail the real exam. In AI, this is called 'Overfitting'. What is the main characteristic of overfitting?",
    "options": [
      "Low performance on both training and new/unseen data",
      "High performance on both training and new/unseen data",
      "High performance on training data, low performance on new/unseen data",
      "Low performance on training data, high on new data"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Overfitting means the model performs excellently on training data (memorized it) but poorly on new, unseen data (can't generalize). It's like a student who memorizes practice answers but can't solve new problems — they aced the practice but failed the real test.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "That describes underfitting, not overfitting! Underfitting means the model is too simple to learn even the training data patterns — low performance on everything. Overfitting is the opposite extreme: the model is too complex and memorizes training data perfectly but fails on new data.",
      "1": "That would be the ideal outcome — a model that performs well on both training and new data! This is called good generalization, and it's what we aim for. Overfitting specifically means high training performance but low performance on new data — the model memorized rather than learned.",
      "2": "Spot on! Overfitting is one of the most common problems in AI. The model essentially 'memorizes' the training examples (high training score) instead of learning the general underlying patterns. When faced with new, slightly different data, it fails (low test score). Just like the student who memorized specific practice answers — they couldn't apply the knowledge to new questions. Detecting and preventing overfitting (through validation sets, regularization, dropout, etc.) is a core skill in machine learning.",
      "3": "That pattern (low training, high test) would be very unusual and counterintuitive — how could a model do better on data it's never seen? In practice, this doesn't really happen. Overfitting is the reverse: high training performance, low test performance. The model has memorized the training data but can't generalize to new examples."
    }
  },
  {
    "id": 60,
    "question": "You wrote a loop: `for i in range(3): print(i)`. What does this code output? Ignore output formatting",
    "options": [
      "0 1 2",
      "1 2 3",
      "0 1 2 3",
      "1 2 3 4"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! `range(3)` generates numbers starting from 0 up to (but not including) 3, producing 0, 1, 2. Python counting starts at 0, and the end value is exclusive — two important rules to remember!",
    "topic": "Programming",
    "subtopic": "Python",
    "feedback_map": {
      "0": "That's right! `range(3)` produces three numbers: 0, 1, 2. Two key Python rules to remember: (1) counting starts at 0, and (2) the stop value is exclusive (not included). So `range(3)` gives 0, 1, 2 — three numbers total, but not 3 itself. This is consistent with 0-based indexing throughout Python.",
      "1": "Close, but off by one! Python's `range()` starts at 0, not 1. So `range(3)` produces 0, 1, 2 — not 1, 2, 3. If you wanted 1, 2, 3, you'd write `range(1, 4)`. This is one of the most common beginner mistakes in Python!",
      "2": "Not quite! `range(3)` produces three numbers (0, 1, 2), not four. The number 3 itself is excluded — Python ranges go up to but don't include the stop value. So you get 0, 1, 2. If you wanted 0, 1, 2, 3, you'd write `range(4)`.",
      "3": "Not quite — Python starts counting at 0, not 1! `range(3)` gives 0, 1, 2. If you wanted 1, 2, 3, 4, you'd write `range(1, 5)`. Remember: `range(n)` starts at 0 and stops before n, producing exactly n numbers."
    }
  },
  {
    "id": 61,
    "question": "Which of the following is a correct way to define a function `predict` that takes an image `x` as input?",
    "options": [
      "func predict(x) ->",
      "define predict(x)",
      "def predict(x):",
      "function predict(x):"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Python uses the `def` keyword to define functions, followed by the function name, parentheses with arguments, and a colon. This syntax is unique to Python — other languages use different keywords like `function`, `func`, or `define`.",
    "topic": "Programming",
    "subtopic": "Python",
    "feedback_map": {
      "0": "`func` is used in Go and Swift, not Python. Python uses `def` — short for 'define.' The correct syntax is `def predict(x):` with a colon at the end. Every Python function definition follows this pattern: `def` + name + `(arguments):` + indented body.",
      "1": "`define` is used in some languages like Scheme, but not Python. Python uses the shorter keyword `def`. The correct syntax is `def predict(x):` — always with a colon at the end to start the function body.",
      "2": "That's right! Python's function syntax is: `def` keyword + function name + `(parameters)` + colon. So `def predict(x):` means 'define a function called predict that takes one input x.' The function body goes on the next lines, indented. This is one of the most fundamental Python patterns you'll use when building AI models!",
      "3": "`function` is the keyword used in JavaScript, not Python. Python's keyword is `def` (short for 'define'). While both languages use a colon-like structure, Python specifically requires `def predict(x):` to define a function."
    }
  },
  {
    "id": 62,
    "question": "If `x = 5` and `y = 10`, what is the result of `x > 3 and y < 5`?",
    "options": [
      "True",
      "10",
      "5",
      "False"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! `x > 3` is True (5 > 3), but `y < 5` is False (10 is not less than 5). The `and` operator requires both conditions to be True. Since one is False, the whole expression evaluates to False.",
    "topic": "Programming",
    "subtopic": "Python",
    "feedback_map": {
      "0": "Not quite! Let's evaluate step by step: `x > 3` → 5 > 3 → True. `y < 5` → 10 < 5 → False. The `and` operator requires BOTH sides to be True. True AND False = False. Even though the first condition passes, the second one fails, making the entire expression False.",
      "1": "The expression evaluates to a boolean (True/False), not a number. `x > 3 and y < 5` compares values and uses a logical operator. Step by step: 5 > 3 is True, 10 < 5 is False, True AND False = False. Python returns the boolean result, not one of the original numbers.",
      "2": "Same as above — this expression produces a boolean, not a number value. The `and` operator combines two True/False comparisons: `x > 3` (True) and `y < 5` (False). Since AND requires both to be True and one is False, the result is False.",
      "3": "Exactly! Step by step: `x > 3` evaluates to True (since 5 > 3). `y < 5` evaluates to False (since 10 is not less than 5). The `and` operator is like a strict gatekeeper — it requires both conditions to be True. Since one is False, the whole expression is False. This is fundamental logic used in every programming language and in AI decision-making!"
    }
  },
  {
    "id": 63,
    "question": "In standard Python, `[1, 2] + [3, 4]` results in `[1, 2, 3, 4]`. In NumPy, if you have arrays `a = np.array([1, 2])` and `b = np.array([3, 4])`, what does `a + b` usually result in?",
    "options": [
      "[1, 2, [3, 4]]",
      "[4, 6]",
      "[1, 2, 3, 4]",
      "Error"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! NumPy arrays perform element-wise arithmetic: [1,2] + [3,4] → [1+3, 2+4] = [4,6]. This is different from Python lists, which concatenate when using +. NumPy's element-wise math is what makes it essential for AI computations.",
    "topic": "Programming",
    "subtopic": "NumPy",
    "feedback_map": {
      "0": "This isn't how NumPy works! [1, 2, [3, 4]] would be nesting, which doesn't happen with NumPy addition. NumPy's + operator does element-wise arithmetic: [1+3, 2+4] = [4, 6]. Each element is added to its corresponding partner.",
      "1": "That's right! NumPy arrays do math element-wise: 1+3=4, 2+4=6 → [4, 6]. This is fundamentally different from Python lists, where `+` concatenates (joins) them into [1, 2, 3, 4]. NumPy's element-wise operations are what make it the backbone of scientific computing and AI — imagine doing matrix math on millions of numbers without writing loops!",
      "2": "That's what Python lists do, not NumPy arrays! Regular Python lists concatenate with +: [1,2] + [3,4] = [1,2,3,4]. But NumPy arrays perform element-wise arithmetic: [1+3, 2+4] = [4,6]. This difference is one of the main reasons NumPy was created — to make mathematical operations on arrays fast and intuitive.",
      "3": "No error! NumPy arrays of the same shape can be added without any issues. The + operator performs element-wise addition: [1+3, 2+4] = [4, 6]. You'd only get an error if the arrays had incompatible shapes (different sizes)."
    }
  },
  {
    "id": 64,
    "question": "You have a 2D array (matrix) representing a game board. If you want to change the shape from 4 rows of 4 columns (4x4) into a single long line of 16 numbers, what operation is this called?",
    "options": [
      "Extending",
      "Flattening / Reshaping",
      "Sorting / Ordering",
      "Expanding"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! Flattening (or reshaping to 1D) converts a multi-dimensional structure into a single long sequence. A 4×4 grid of 16 numbers becomes a single row of 16 numbers. In AI, flattening is commonly used to convert image data into a format that fully connected neural network layers can process.",
    "topic": "Programming",
    "subtopic": "NumPy",
    "feedback_map": {
      "0": "Extending typically means adding more data to an existing structure (like appending elements to a list), not changing its shape. Converting a 4×4 grid into a 1×16 sequence is called Flattening or Reshaping — the same data is rearranged into a different shape without adding or removing anything.",
      "1": "Exactly! Flattening takes a multi-dimensional array and 'unrolls' it into a single dimension. A 4×4 grid (2D) becomes a 1D list of 16 numbers. In NumPy, you can do this with `array.flatten()` or `array.reshape(16)`. This operation is crucial in AI: after convolutional layers extract features from an image grid, the data is often flattened before passing to fully connected layers.",
      "2": "Sorting changes the order of elements (like arranging numbers from smallest to largest), which is a completely different operation. Converting a 4×4 grid to a 1×16 sequence doesn't change any values or their relative order — it just changes the shape. This is called Flattening or Reshaping.",
      "3": "Expanding adds new dimensions (like converting a 4×4 grid to a 4×4×1 volume), which is the opposite of what we want. We're reducing dimensions — going from 2D (4×4) to 1D (16 elements). This is called Flattening."
    }
  },
  {
    "id": 65,
    "question": "If you multiply a NumPy array by 2, like `arr * 2`, what happens to the numbers inside?",
    "options": [
      "The array is duplicated",
      "The array becomes twice as long",
      "Every number inside is multiplied by 2",
      "Only the odd numbers are multiplied by 2"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! NumPy's broadcasting applies the scalar operation to every element. `arr * 2` doubles each number in the array. This is much faster than writing a loop, and it's one of the key reasons NumPy is used for AI computations.",
    "topic": "Programming",
    "subtopic": "NumPy",
    "feedback_map": {
      "0": "That's what happens with Python lists! `[1,2,3] * 2` gives `[1,2,3,1,2,3]` — the list is repeated. But NumPy arrays behave differently: `np.array([1,2,3]) * 2` gives `[2,4,6]` — each element is multiplied. This distinction between list replication and NumPy element-wise math is important to remember!",
      "1": "That's Python list behavior, not NumPy! Python lists duplicate when multiplied (`[1,2] * 2` → `[1,2,1,2]`). NumPy arrays do math: `np.array([1,2]) * 2` → `[2,4]`. The array stays the same length, but every value inside is doubled.",
      "2": "That's right! NumPy's broadcasting feature applies the scalar operation (× 2) to every single element in the array. [1, 2, 3] * 2 = [2, 4, 6]. This is incredibly powerful for AI: instead of writing slow Python loops, NumPy handles the math for all elements simultaneously using optimized C code. It's one of the reasons AI in Python is so fast!",
      "3": "NumPy doesn't selectively operate on some elements — scalar multiplication applies to every element equally. `arr * 2` doubles each number in the entire array, regardless of whether values are odd or even. Broadcasting is universal: the same operation hits every element."
    }
  },
  {
    "id": 66,
    "question": "In Pandas, data is organized in rows and columns, similar to an Excel sheet. What is this primary data structure called? In machine learning, this kind of computation commonly appears when working with feature vectors, embeddings, or gradients.",
    "options": [
      "DataFrame",
      "Dictionary",
      "Tensor",
      "List"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! The DataFrame is Pandas' primary data structure — a 2D table with labeled rows and columns, similar to an Excel spreadsheet. It's the go-to structure for organizing, cleaning, and analyzing tabular data in AI and data science.",
    "topic": "Programming",
    "subtopic": "Pandas",
    "feedback_map": {
      "0": "Exactly! The DataFrame is the heart of the Pandas library. It organizes data into rows and columns with labels, just like a spreadsheet. You can easily select columns, filter rows, calculate statistics, and merge datasets. In AI, DataFrames are typically the first stop for loading and preprocessing training data before feeding it to a model.",
      "1": "A Dictionary (in Python, `dict`) stores key-value pairs like {'name': 'Alice', 'age': 16}. While DataFrames use similar column-name access, they're much more powerful — they handle entire tables of data with many rows and columns, support filtering, grouping, merging, and statistical operations. The correct answer is DataFrame.",
      "2": "Tensors are the primary data structure in deep learning frameworks like PyTorch and TensorFlow — they're multi-dimensional arrays optimized for GPU computation. Pandas' main structure is the DataFrame, which is specifically designed for tabular (spreadsheet-like) data with labeled rows and columns.",
      "3": "A Python List is a basic ordered collection like `[1, 2, 3]`. It doesn't have column labels, row indices, or built-in data analysis functions. Pandas' DataFrame is specifically designed for tabular data with rows, columns, and labels — much more like a spreadsheet than a simple list."
    }
  },
  {
    "id": 67,
    "question": "You have a Pandas DataFrame `df` with columns 'Age' and 'Name'. How do you select just the 'Age' column?",
    "options": [
      "df.select('Age')",
      "df(Age)",
      "df[0]",
      "df['Age']"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! `df['Age']` uses bracket notation with the column name as a string to select a single column from the DataFrame. This is one of the most common operations in data science!",
    "topic": "Programming",
    "subtopic": "Pandas",
    "feedback_map": {
      "0": "`.select()` is not a standard Pandas method for column access (it's more of an SQL concept). Pandas uses bracket notation: `df['Age']`. You can also use dot notation (`df.Age`), but bracket notation is more reliable, especially when column names contain spaces or special characters.",
      "1": "`df(Age)` isn't valid Python syntax for DataFrames. Parentheses after `df` would try to call it as a function. The correct syntax uses square brackets with a string: `df['Age']`. This follows the dictionary-style access pattern that Pandas uses for columns.",
      "2": "`df[0]` doesn't select a column by number in Pandas — it would either try to select a row or raise an error, depending on the context. To select the 'Age' column, use its name: `df['Age']`. Pandas primarily uses column names (not numeric indices) for column selection.",
      "3": "That's right! `df['Age']` is the standard way to access a column in Pandas. The column name goes inside quotes within square brackets. This returns a Pandas Series (a single column of data) that you can then analyze, plot, or transform. You'll use this pattern constantly in data science and AI — it's one of the most fundamental Pandas operations!"
    }
  },
  {
    "id": 68,
    "question": "In a scatter plot of 'House Price' vs 'Size', you see points forming a line going up and to the right. What does this suggest?",
    "options": [
      "The data is random",
      "There is no relationship",
      "As Size increases, Price increases",
      "As Size increases, Price decreases"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! Points forming an upward line from left to right indicate a positive correlation: as one variable (Size) increases, the other (Price) also increases. This is the simplest type of pattern to model with linear regression in AI.",
    "topic": "Programming",
    "subtopic": "Matplotlib",
    "feedback_map": {
      "0": "If the data were random, the points would be scattered all over the plot with no clear pattern or direction. An upward line from left to right shows a clear, non-random pattern — specifically, a positive relationship between the two variables.",
      "1": "There clearly is a relationship! Points forming a line indicates a strong correlation between the variables. 'No relationship' would look like randomly scattered points with no directional trend. The upward direction specifically tells us it's a positive relationship.",
      "2": "Exactly! This is a positive correlation — as Size gets bigger, Price also gets bigger. The data points trend upward from left to right, suggesting larger houses cost more. In AI, this is exactly the kind of pattern that linear regression can capture: drawing a 'best fit' line through the data to predict prices for houses of any given size.",
      "3": "That would be a negative correlation — an upward-left-to-lower-right trend. The question describes an upward trend (lower-left to upper-right), which means both variables increase together. When size goes up, price goes up too — that's a positive correlation."
    }
  },
  {
    "id": 69,
    "question": "Why do we often use Matplotlib to visualize the 'Loss' (Error) during training?",
    "options": [
      "To see if the model is actually learning",
      "To make the AI processing visible",
      "To make the code converge and run faster",
      "To save memory when minimizing errors"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Visualizing the loss curve is essential for diagnosing training progress. If the loss decreases, the model is learning. If it's flat, learning has stalled. If it increases, something is wrong. This quick visual check saves hours of debugging!",
    "topic": "Programming",
    "subtopic": "Matplotlib",
    "feedback_map": {
      "0": "Exactly! The loss curve is like a health monitor for your AI model. A steadily decreasing loss means the model is learning from the data. A flat line means it's stuck. A rising line or wild oscillations signal problems (like a bad learning rate). Data scientists check this plot constantly during training — it's the fastest way to tell if something needs to be adjusted.",
      "1": "Visualization doesn't make the AI run visibly — it's a diagnostic tool for the engineer. The loss plot doesn't show the AI 'thinking.' It shows a numerical metric (error) over time, helping you decide if training is going well or if you need to change settings like learning rate, model architecture, or data preprocessing.",
      "2": "Visualization doesn't affect the code's execution speed or convergence at all! The loss curve is purely a monitoring tool — it shows you what's happening, but doesn't change the training process. It's like a speedometer in a car: it tells you how fast you're going but doesn't make the car go faster.",
      "3": "Visualization has nothing to do with memory usage! Matplotlib creates a chart that helps you understand training progress. The loss plot is a diagnostic tool — it helps you detect problems (like overfitting or a bad learning rate) early, saving you from wasting time on a model that isn't learning."
    }
  },
  {
    "id": 70,
    "question": "In PyTorch, the fundamental data structure is a 'Tensor'. How is it different from a standard number list?",
    "options": [
      "It can hold both integers and floating point numbers",
      "It is slower but more accurate",
      "It is fundamentally the same but has built-in optimization",
      "It can run on a GPU (Graphics Card) for faster calculation"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! The key advantage of PyTorch Tensors over Python lists is GPU compatibility. Tensors can be moved to a GPU for massively parallel computation, making matrix operations thousands of times faster — essential for training deep learning models.",
    "topic": "Programming",
    "subtopic": "PyTorch",
    "feedback_map": {
      "0": "Python lists can also hold mixed types, so this isn't what makes Tensors special. The key difference is GPU acceleration: Tensors can be moved to a GPU to perform thousands of calculations in parallel. This is what makes deep learning feasible — the same math on CPU lists would take days instead of hours.",
      "1": "Actually, it's the opposite! Tensors are typically faster than lists for mathematical operations, especially on GPUs. The key advantage isn't accuracy — it's speed through GPU parallelism. Deep learning involves billions of calculations, and GPUs can handle them thousands of times faster than CPUs.",
      "2": "While Tensors do have some built-in optimizations (like automatic differentiation), the most important difference is GPU compatibility. Tensors can run on GPUs (Graphics Processing Units), which perform parallel matrix math thousands of times faster than CPUs. This is the fundamental reason deep learning uses Tensors, not regular lists.",
      "3": "That's right! The game-changing feature of Tensors is GPU compatibility. GPUs have thousands of cores designed for parallel math, making them perfect for the massive matrix operations in deep learning. A computation that takes hours on a CPU with Python lists can take minutes on a GPU with Tensors. This is why every deep learning framework (PyTorch, TensorFlow) is built around GPU-accelerated tensors!"
    }
  },
  {
    "id": 71,
    "question": "When training a neural network, we calculate 'gradients'. What is the purpose of a gradient in this context?",
    "options": [
      "To save the model so that it can remember more context",
      "To increase the data size to accommodate larger datasets",
      "To change the filters and colors of the image",
      "To tell the model which direction to adjust its weights to reduce error"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! The gradient tells the model the direction and magnitude of the steepest increase in error. By moving in the opposite direction (gradient descent), the model adjusts its weights to reduce the error step by step.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "Gradients aren't about saving or storing the model! They're mathematical signals that guide learning. The gradient tells each weight 'you contributed this much to the error — adjust by this amount in this direction.' Think of it like a GPS telling you which way to turn to reach your destination (lower error).",
      "1": "Gradients don't change the data size! They're computed from the data to guide the model's learning. Each gradient value tells a specific weight how much it needs to change (and in which direction) to reduce the error. It's the core mechanism that makes neural networks learn.",
      "2": "Gradients have nothing to do with image filters or colors! In neural networks, gradients are mathematical derivatives that tell each weight parameter how to change to reduce the model's error. They point 'uphill' (toward more error), so we move in the opposite direction (downhill) to minimize error.",
      "3": "Exactly! Gradients are the compass of neural network training. They point in the direction where error increases most steeply. By moving in the opposite direction (Gradient Descent), the model adjusts its weights to reduce error. This is backpropagation in action: compute the gradient for every weight, then update each weight in the direction that reduces error. It's how AI learns!"
    }
  },
  {
    "id": 72,
    "question": "You see code `x.requires_grad = True`. What does this tell PyTorch to do? In machine learning, this kind of computation commonly appears when working with feature vectors, embeddings, or gradients.",
    "options": [
      "Make x unchangeable",
      "Print x to the screen",
      "Track all operations on x so we can calculate gradients later",
      "Delete x after use"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! `requires_grad = True` enables PyTorch's Autograd system, which records all operations on that tensor. This allows automatic gradient computation later when you call `backward()` — essential for training neural networks.",
    "topic": "Programming",
    "subtopic": "PyTorch",
    "feedback_map": {
      "0": "The opposite! `requires_grad = True` doesn't make x unchangeable — it makes PyTorch pay extra attention to x by tracking all operations performed on it. This tracking builds a computation graph that enables automatic gradient calculation during backpropagation. The tensor remains fully modifiable.",
      "1": "This flag has nothing to do with printing! `requires_grad = True` tells PyTorch to record the computation graph — tracking every mathematical operation applied to x. This is necessary for backpropagation: when you later call `backward()`, PyTorch can trace back through all operations to compute gradients automatically.",
      "2": "That's right! `requires_grad = True` activates PyTorch's Autograd (automatic gradient) system for this tensor. PyTorch will then record every operation (addition, multiplication, etc.) applied to x, building a computation graph. When you later call `loss.backward()`, PyTorch traverses this graph backwards to compute the gradient (derivative) for every tracked tensor. This automatic differentiation is what makes training neural networks practical!",
      "3": "It's not about deletion! `requires_grad = True` does the opposite of disposing the tensor — it tells PyTorch to carefully track all operations on x. This extra bookkeeping enables automatic gradient computation during backpropagation, which is essential for training the neural network."
    }
  },
  {
    "id": 73,
    "question": "In PyTorch, a 'Linear' layer `nn.Linear(10, 5)` transforms data. If you input a vector of size 10, what size is the output vector?",
    "options": [
      "15",
      "5",
      "50",
      "10"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! `nn.Linear(10, 5)` creates a layer that transforms a 10-dimensional input into a 5-dimensional output. The first number is the input size, the second is the output size. Internally, this uses a 10×5 weight matrix plus a 5-dimensional bias vector.",
    "topic": "Programming",
    "subtopic": "PyTorch",
    "feedback_map": {
      "0": "15 would be 10 + 5, but that's not how linear layers work. `nn.Linear(10, 5)` maps from 10 inputs to 5 outputs — it reduces the dimension, not adds them. The output vector has exactly 5 elements.",
      "1": "That's right! `nn.Linear(in_features=10, out_features=5)` takes a vector of 10 numbers and transforms it into a vector of 5 numbers using a weight matrix and bias. Think of it as compressing information: 10 input features are combined and condensed into 5 output features. Inside, it multiplies by a 10×5 weight matrix and adds a bias vector of size 5.",
      "2": "50 is the number of weight parameters in the layer (10 inputs × 5 outputs = 50 connections), not the output size! The output vector has exactly 5 elements, as specified by the second argument. `nn.Linear(10, 5)` maps 10 → 5.",
      "3": "10 is the input size, not the output size. `nn.Linear(10, 5)` transforms 10-dimensional input into 5-dimensional output. The transformation reduces the dimension from 10 to 5. If you wanted the output to stay at 10, you'd write `nn.Linear(10, 10)`."
    }
  },
  {
    "id": 74,
    "question": "After calculating the loss (error), we call `loss.backward()`. What does this step do?",
    "options": [
      "It terminates and program and shuts down the computer",
      "It calculates the gradients (how much each parameter contributed to the error)",
      "It restarts the training when a threshold is reached",
      "It activates the appropriate AI agent"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! `loss.backward()` triggers backpropagation — the algorithm that computes how much each parameter (weight) in the network contributed to the error. These gradients are then used to update the weights in the right direction.",
    "topic": "Programming",
    "subtopic": "PyTorch",
    "feedback_map": {
      "0": "That's not what backward() does! It doesn't stop or shut down anything. `backward()` performs backpropagation — computing the gradient of the loss with respect to every learnable parameter in the network. Training continues after this step (next comes the optimizer step that actually updates the weights).",
      "1": "Exactly! `loss.backward()` triggers Backpropagation, one of the most important algorithms in deep learning. It traces backward through the network's computation graph, computing how much each weight contributed to the final error (the gradient). These gradients are like a report card for each parameter: 'here's how much you need to change, and in which direction, to reduce the error.' The optimizer then uses these gradients to actually update the weights.",
      "2": "`backward()` doesn't restart or reset anything. It computes gradients — the mathematical derivatives that tell each parameter how to change to reduce error. After backward(), the next step is usually `optimizer.step()` (which applies the updates) and `optimizer.zero_grad()` (which resets gradients for the next iteration).",
      "3": "`backward()` has nothing to do with AI agents! It's a mathematical operation that computes gradients through backpropagation. The gradients tell each weight in the network how much it needs to change (and in which direction) to reduce the loss. It's pure calculus, automated by PyTorch."
    }
  },
  {
    "id": 75,
    "question": "You try to run your code and see `IndentationError: expected an indented block`. What is the likely cause?",
    "options": [
      "You misspelled a variable by including extra spaces since Python is white space sensitive",
      "You are using an incompatible Python version on the wrong architecture",
      "Your computer is out of memory because the hard disk is full",
      "You forgot a colon or did not use spaces/tabs correctly inside a loop or function"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Python uses indentation (spaces or tabs) to define code blocks inside loops, functions, and if-statements. Forgetting to indent after a colon, or mixing tabs and spaces, triggers an `IndentationError`. This is unique to Python — most other languages use braces {} instead.",
    "topic": "Programming",
    "subtopic": "Debugging",
    "feedback_map": {
      "0": "Misspelling a variable name would cause a `NameError`, not an `IndentationError`. The indentation error specifically means Python expected an indented code block (after `def`, `if`, `for`, `while`, etc.) but didn't find one. Check that lines after colons are properly indented with consistent spaces or tabs.",
      "1": "Python version or architecture mismatches would cause different errors (like `SyntaxError` for truly incompatible syntax or import errors). An `IndentationError` is specifically about whitespace — Python uses indentation to define code structure. Make sure you indent consistently after colons (`:`).",
      "2": "Memory issues would cause `MemoryError` or `OSError`, not `IndentationError`. Indentation errors are purely about code formatting: Python expects an indented block after statements ending with a colon (like `def`, `if`, `for`). It's a code structure issue, not a hardware issue.",
      "3": "That's right! Python is unique in using indentation to define code structure. After any line ending with a colon (`:`) — like `def`, `if`, `for`, `while`, `class` — the next line must be indented. Common causes of this error: forgetting to indent, mixing tabs and spaces, or having an empty block without a `pass` statement. Pro tip: configure your editor to use 4 spaces for indentation to avoid tab/space mixing!"
    }
  },
  {
    "id": 76,
    "question": "You try to access the 5th element of a list that only has 3 items. What error do you expect?",
    "options": [
      "IndexError: list index out of range",
      "KeyError",
      "ValueError",
      "SyntaxError"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! Accessing an index beyond the list's length raises an `IndexError`. A 3-element list has valid indices 0, 1, 2 — trying to access index 4 (the '5th' element) is out of range.",
    "topic": "Programming",
    "subtopic": "Debugging",
    "feedback_map": {
      "0": "Exactly! A 3-element list has indices 0, 1, and 2. Trying to access index 4 (the '5th' element) is out of range, so Python raises `IndexError: list index out of range`. This is one of the most common errors in programming. Always check your list length before accessing elements by index!",
      "1": "`KeyError` occurs when you try to access a non-existent key in a dictionary (like `my_dict['nonexistent']`). For lists, accessing an invalid index raises `IndexError`, not `KeyError`. Lists use numeric indices, dictionaries use keys — different structures, different error types.",
      "2": "`ValueError` occurs when a function receives an argument of the right type but inappropriate value (like `int('hello')`). Accessing a non-existent list index raises `IndexError` specifically. Each error type in Python describes a specific kind of problem, which helps with debugging.",
      "3": "`SyntaxError` means the code itself is grammatically incorrect (like missing parentheses or typos in keywords). If the code runs but crashes at runtime when accessing the list, it's an `IndexError`. Syntax errors are caught before the code runs; IndexErrors happen during execution when an invalid index is encountered."
    }
  },
  {
    "id": 77,
    "question": "A decision tree splits data based on rules. If the first rule is 'Is Age > 10?', and the 'Yes' path leads to a leaf node saying 'Class: Student', what happens to a 12-year-old input?",
    "options": [
      "It is classified as 'Not Student'",
      "It is discarded",
      "It is classified as 'Student' immediately",
      "It goes to the next rule"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! A 12-year-old satisfies 'Age > 10' (Yes), so it follows the Yes path. Since that path leads directly to a leaf node (final decision) saying 'Student,' the classification is immediate — no further rules are checked.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "'Not Student' would be the result if the answer to 'Age > 10?' were No. Since 12 > 10 is True (Yes), the input follows the Yes branch, which leads to the 'Student' leaf node. The classification is 'Student,' not 'Not Student.'",
      "1": "Data is never discarded in a decision tree! Every input follows a path from the root to a leaf node and receives a classification. A 12-year-old satisfies 'Age > 10' and follows the Yes path to the 'Student' leaf.",
      "2": "Correct! 12 > 10 is True, so the input follows the 'Yes' path. Since this path leads to a leaf node (a final decision, not another question), the classification is 'Student' immediately. In decision trees, leaf nodes are endpoints — once you reach one, you have your answer. No further splitting or rules are applied.",
      "3": "Not in this case! You only go to the next rule if the current path leads to another decision node (another question). Here, the 'Yes' path leads directly to a leaf node ('Student'), which is a final decision. Leaf nodes = stop here, you have your answer. Only internal (non-leaf) nodes contain additional rules."
    }
  },
  {
    "id": 78,
    "question": "A simple neuron has 2 inputs: x1=2, x2=3. The weights are w1=0.5, w2=1.0. The bias is b=-1. The calculation is (x1*w1 + x2*w2 + b). What is the output before activation?",
    "options": [
      "3",
      "0",
      "-1",
      "4"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! The weighted sum is: (2 × 0.5) + (3 × 1.0) + (-1) = 1 + 3 - 1 = 3. This is the pre-activation output of a single neuron — the fundamental building block of neural networks.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "That's right! Step by step: x1×w1 = 2×0.5 = 1, x2×w2 = 3×1.0 = 3, then add the bias: 1 + 3 + (-1) = 3. This weighted sum is what every neuron computes before applying an activation function. Understanding this calculation is the foundation of understanding neural networks — every neuron in a network performs this same basic operation!",
      "1": "Not quite! Let's trace through the math carefully: (2 × 0.5) = 1, (3 × 1.0) = 3, then add bias (-1): 1 + 3 + (-1) = 3, not 0. You might have made an arithmetic error. The answer is 3.",
      "2": "Check the math! (2 × 0.5) = 1, (3 × 1.0) = 3. The sum of weighted inputs is 1 + 3 = 4. Adding the bias: 4 + (-1) = 3, not -1. The bias of -1 is added to the sum, not used as the final answer.",
      "3": "Close but not quite! 4 would be the answer if you forgot to add the bias. The full calculation is: (2 × 0.5) + (3 × 1.0) + bias = 1 + 3 + (-1) = 3. Don't forget the bias term — it's added at the end and shifts the output up or down."
    }
  },
  {
    "id": 79,
    "question": "You have a neural network with an input layer of size 100 (for a 10x10 image) and a hidden layer of size 50. How many weight parameters connect these two layers? (Ignore bias)",
    "options": [
      "100",
      "5000",
      "150",
      "50"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Correct! In a fully connected layer, every input neuron connects to every output neuron. With 100 inputs and 50 outputs, that's 100 × 50 = 5000 weight parameters. This is why neural networks can have millions of parameters!",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "100 is just the size of the input layer, not the number of weights. In a fully connected layer, every input connects to every output neuron. Since there are 100 inputs and 50 outputs, the total connections (weights) = 100 × 50 = 5000.",
      "1": "Exactly! In a fully connected (dense) layer, each of the 100 input neurons has a separate connection (weight) to each of the 50 output neurons. That's 100 × 50 = 5000 individual weights! This is why even small neural networks can have thousands of parameters, and large ones can have billions. Each weight is a learnable number that gets adjusted during training.",
      "2": "150 is the sum of the layer sizes (100 + 50), not the number of connections. In a fully connected layer, the number of weights is the product (multiplication), not the sum: 100 × 50 = 5000. Every input connects to every output.",
      "3": "50 is the size of the hidden layer, not the number of weights. In a fully connected layer, each of the 100 inputs connects to each of the 50 outputs: 100 × 50 = 5000 weights total. Think of it as a grid: 100 rows × 50 columns = 5000 intersections."
    }
  },
  {
    "id": 80,
    "question": "During training, your Loss curve (Error) bounces up and down wildly and never settles at a low number. What is the most likely cause?",
    "options": [
      "The Learning Rate is too high (taking steps that are too big)",
      "The Learning Rate is too low (taking tiny steps)",
      "You have too much data",
      "The model needs more data for training"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Correct! A wildly bouncing loss curve is the classic symptom of a learning rate that is too high. The optimizer takes steps so large that it overshoots the minimum, bouncing back and forth across the valley of the loss landscape.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "That's right! Wild oscillations in the loss curve are the telltale sign of an excessively high learning rate. The optimizer overshoots the minimum on each step, landing on the other side and bouncing back. It's like trying to park a car using only full-throttle acceleration — you keep zooming past the spot! The fix: reduce the learning rate so the steps become smaller and more controlled.",
      "1": "The opposite symptom! A too-low learning rate would produce a very slow, steady decline — the loss would decrease smoothly but painfully slowly. Wild bouncing indicates steps that are too large (learning rate too high), not too small. If your loss curve looks smooth but slow, try increasing the learning rate.",
      "2": "Having too much data doesn't cause loss oscillation! More data generally helps training be more stable (smoother gradients). Wild bouncing is almost always caused by a learning rate that's too high — each update overshoots the target, causing the loss to jump up and down chaotically.",
      "3": "Needing more data typically causes underfitting (high loss that plateaus), not wild oscillations. The bouncing pattern specifically indicates that the optimizer's steps are too large (learning rate too high). Each update overshoots the minimum, and the loss swings wildly. More data wouldn't fix this — reducing the learning rate would."
    }
  },
  {
    "id": 81,
    "question": "You train a neural network to recognize handwritten digits. It works perfectly on white digits on a black background (training data). You test it on black digits on a white background, and it fails completely. Why?",
    "options": [
      "The numbers are different",
      "The computer is rule-based and cannot adapt to noisy inputs",
      "The network requires GPU to accommodate different backgrounds",
      "The model only learned specific pixel patterns"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! The model learned to associate specific pixel intensity patterns (white=digit, black=background) rather than the abstract shape of the digits. When colors invert, the learned patterns no longer match, and the model fails. This shows the importance of training on diverse data.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "The digits are the same shapes — a '3' is still a '3' regardless of color. The problem isn't different numbers; it's that the model learned pixel brightness patterns instead of shapes. It associates 'bright pixels' with digits, so when brightness is inverted, it sees everything backwards.",
      "1": "Neural networks aren't rule-based systems — they learn from data. The issue isn't an inability to adapt to noise, but rather that the training data only showed one color scheme. The model learned that 'bright = digit' instead of learning the actual shapes. More diverse training data (including inverted images) would fix this.",
      "2": "GPUs don't fix conceptual problems! The model's failure is about what it learned (pixel brightness patterns), not computational power. Even the world's fastest GPU can't help a model that learned the wrong features. The fix is better training data or data augmentation (including inverted images).",
      "3": "Exactly! The model took a shortcut: instead of learning the shape of each digit, it learned that bright pixels = digit and dark pixels = background. When colors invert, this 'shortcut' completely breaks. This is called learning spurious features. The fix: data augmentation — training with both normal and inverted images forces the model to learn actual shapes rather than brightness patterns. This is a common pitfall in computer vision!"
    }
  },
  {
    "id": 82,
    "question": "A ReLU activation function outputs the input if it's positive, and 0 if it's negative. If the input vector is [-5, 10, -2], what is the output vector?",
    "options": [
      "[5, 10, 2]",
      "[-5, 10, -2]",
      "[0, 10, 0]",
      "[0, 0, 0]"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Correct! ReLU (Rectified Linear Unit) outputs the input if positive, and 0 if negative: max(0, x). So: ReLU(-5)=0, ReLU(10)=10, ReLU(-2)=0 → [0, 10, 0]. ReLU is the most widely used activation function in deep learning.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "[5, 10, 2] would be the result of applying absolute value (|x|), not ReLU. Absolute value makes negatives positive, while ReLU replaces negatives with zero. Key difference: ReLU(-5) = 0, but |−5| = 5. This distinction matters because ReLU introduces 'sparsity' — many neurons output exactly zero.",
      "1": "[-5, 10, -2] would be the identity function (output = input with no changes). ReLU does modify the input — it replaces negative values with zero. ReLU(-5) = 0, ReLU(10) = 10, ReLU(-2) = 0 → [0, 10, 0].",
      "2": "That's right! ReLU is elegantly simple: if the input is positive, pass it through unchanged; if negative, replace it with 0. Mathematically: ReLU(x) = max(0, x). So: ReLU(-5)=0, ReLU(10)=10, ReLU(-2)=0 → [0, 10, 0]. Despite its simplicity, ReLU is the most popular activation function in deep learning because it's fast to compute and helps gradients flow during training.",
      "3": "[0, 0, 0] would mean all values are zeroed out, which only happens if all inputs are negative (or zero). Here, 10 is positive, so ReLU passes it through: ReLU(10) = 10. Only the negative values (-5 and -2) become 0. Result: [0, 10, 0]."
    }
  },
  {
    "id": 83,
    "question": "You have a 4x4 image (16 pixels). You apply a 2x2 Max Pooling operation with a stride of 2 (no overlap). What is the size of the resulting image?",
    "options": [
      "8x8",
      "1x1",
      "4x4",
      "2x2"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! Max Pooling with a 2×2 window and stride 2 halves each dimension: 4÷2 = 2 in each direction, resulting in a 2×2 output. Pooling reduces spatial dimensions while keeping the most important information (the maximum values in each region).",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "8×8 would be double the original size, but pooling reduces dimensions, not increases them! A 2×2 Max Pooling with stride 2 halves each dimension: 4÷2 = 2. The result is 2×2, not 8×8.",
      "1": "1×1 would mean the entire image is reduced to a single pixel, which would require much more aggressive pooling. A 2×2 window with stride 2 on a 4×4 image creates four non-overlapping 2×2 regions, producing a 2×2 result (4÷2 = 2).",
      "2": "4×4 means no size change, but pooling specifically reduces spatial dimensions! With a 2×2 window and stride 2, the output is half the size in each direction: 4÷2 = 2 in both height and width → 2×2 output.",
      "3": "Exactly! Max Pooling with a 2×2 window and stride 2 divides the image into non-overlapping 2×2 blocks and keeps only the maximum value from each block. A 4×4 image has four such blocks (2 across, 2 down), producing a 2×2 output. This downsampling helps CNNs by reducing computation, adding translation invariance, and focusing on the most prominent features in each region."
    }
  },
  {
    "id": 84,
    "question": "An RGB image pixel is represented by [R, G, B] values from 0-255. You have a pure Blue pixel [0, 0, 255]. You want to make it White [255, 255, 255]. What do you need to add?",
    "options": [
      "Just Red",
      "Black",
      "Just Green",
      "Red and Green"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Correct! White = [255, 255, 255]. Starting from blue [0, 0, 255], you need to add red (R=255) and green (G=255) — effectively adding [255, 255, 0]. This demonstrates additive color mixing in the RGB color model used in digital images.",
    "topic": "AI-ML-DL",
    "subtopic": "Computer Vision",
    "feedback_map": {
      "0": "Adding just Red would give [255, 0, 255] — that's Magenta (a pink-purple color), not White! To reach White [255, 255, 255] from Blue [0, 0, 255], you need to fill in both the Red AND Green channels. White requires all three channels at maximum.",
      "1": "Black is [0, 0, 0] — adding it would change nothing! In RGB, Black means 'no light' in all channels. To go from Blue [0, 0, 255] to White [255, 255, 255], you need to add Red and Green light, not Black.",
      "2": "Adding just Green would give [0, 255, 255] — that's Cyan (a turquoise color), not White! While you've correctly identified that Green is needed, you also need Red. White requires all three channels at maximum: [255, 255, 255].",
      "3": "That's right! In the RGB additive color model: White = [255, 255, 255] = maximum Red + Green + Blue. Starting from Blue [0, 0, 255], you need to add [255, 255, 0] — that's Red and Green. This is additive color mixing: Red + Green + Blue = White. Fun fact: your computer screen creates every color by mixing these three primary lights at different intensities!"
    }
  },
  {
    "id": 85,
    "question": "Object Detection uses 'IoU' (Intersection over Union) to check if a prediction is correct. If the predicted box covers the exact same area as the ground truth box, but is shifted slightly to the right so they overlap 50%, is the IoU 50%?",
    "options": [
      "No, it is more than 50%",
      "It depends on the image",
      "Yes, exactly 50%",
      "No, it is less than 50%"
    ],
    "correctAnswerIndex": 3,
    "explanation": "The IoU formula is Intersection ÷ Union. When two identical boxes overlap by 50%, the Intersection is 0.5 × Area. However, the Union is not just the Area — it's (Area + Area − Intersection) = 1.5 × Area. So IoU = 0.5 / 1.5 = 1/3 ≈ 33%, which is less than 50%. The key insight is that the Union always grows when boxes don't perfectly overlap, which drags the IoU down below the raw overlap percentage.",
    "topic": "AI-ML-DL",
    "subtopic": "Computer Vision",
    "feedback_map": {
      "0": "Not quite — the IoU is actually less than 50%, not more. Since the Union (the total area covered by both boxes combined) is larger than a single box, dividing the overlap by this larger number gives a result smaller than the raw overlap percentage. Work through the math: Intersection = 0.5 × Area, Union = 1.5 × Area, so IoU = 0.5/1.5 ≈ 33%.",
      "1": "IoU is a purely geometric calculation — it depends only on how much the predicted and ground truth boxes overlap relative to their combined area, not on the image content itself. The same box positions will always produce the same IoU regardless of what objects are in the image.",
      "2": "This is the most common mistake! It's tempting to think '50% overlap means 50% IoU,' but the formula is Intersection ÷ Union, not Intersection ÷ Area. The Union includes the non-overlapping parts of both boxes, making it 1.5 × Area. So IoU = 0.5/1.5 = 33%, which is significantly less than 50%.",
      "3": "Excellent! You correctly applied the IoU formula: IoU = Intersection ÷ Union. With 50% overlap, the Intersection = 0.5 × Area and the Union = Area + Area − 0.5 × Area = 1.5 × Area. So IoU = 0.5/1.5 = 1/3 ≈ 33%. This is why even a seemingly good overlap can produce a surprisingly low IoU score — the Union in the denominator always grows when boxes aren't perfectly aligned."
    }
  },
  {
    "id": 86,
    "question": "A standard Convolutional Neural Network (CNN) takes an image as input. As the data goes deeper into the layers (after many convolutions and pooling), what generally happens to the spatial dimensions (Height/Width)?",
    "options": [
      "They become 3D",
      "They get larger",
      "They stay the same",
      "They get smaller"
    ],
    "correctAnswerIndex": 3,
    "explanation": "In a standard CNN, pooling layers (like max pooling) and strided convolutions progressively reduce the height and width of the feature maps. For example, a 224×224 image might shrink to 112×112, then 56×56, and so on. This downsampling helps the network focus on high-level patterns (like shapes and objects) rather than individual pixels, while also reducing the computational cost of deeper layers.",
    "topic": "AI-ML-DL",
    "subtopic": "Neural Networks",
    "feedback_map": {
      "0": "The data in a CNN is actually already 3D from the start — each layer produces a 3D volume (Height × Width × Channels). This question specifically asks about the spatial dimensions (Height and Width), which get smaller as data passes through pooling and strided convolution layers. The number of channels (depth) typically increases, but that's a separate dimension from the spatial ones.",
      "1": "In a standard CNN, spatial dimensions shrink, not grow, as data goes deeper. Pooling layers (e.g., 2×2 max pooling) cut the height and width in half at each step. While there are architectures that upsample (like autoencoders or U-Nets), the question asks about a standard CNN, where progressive downsampling is the norm.",
      "2": "While it's possible to use 'same' padding in convolutions to preserve dimensions temporarily, standard CNNs include pooling layers (like max pooling) that deliberately reduce the spatial size. This shrinking is a core design feature — it forces the network to compress information into increasingly abstract representations.",
      "3": "Correct! Pooling layers and strided convolutions progressively shrink the height and width of feature maps as data moves deeper. For instance, a 224×224 input might become 112×112, then 56×56, and so on. This serves two purposes: it reduces computation and forces the network to learn increasingly abstract, high-level features rather than focusing on fine pixel-level details."
    }
  },
  {
    "id": 87,
    "question": "You need to sort a list of 1,000,000 student grades to find the top 10. You can either (A) Sort the entire list and pick the top 10, or (B) Scan through the list once keeping track of only the top 10 seen so far. Which approach is generally faster (more efficient) for this specific task?",
    "options": [
      "It depends on the grades",
      "Both take the exact same time",
      "Approach A (Full Sort)",
      "Approach B (Scan and Track)"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Approach B only needs to scan the list once (O(N)), comparing each grade to the current top 10 and swapping if needed. Approach A sorts the entire list (O(N log N)), which is much more work since it organises all 1,000,000 elements when you only care about 10 of them. Think of it this way: if you want to find the tallest 10 people in a stadium, you wouldn't line everyone up by height — you'd just walk through the crowd remembering the tallest 10 you've seen.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "The efficiency comparison doesn't depend on the actual grade values. Regardless of whether grades are clustered, spread out, or partially sorted, scanning once (O(N)) is fundamentally faster than a full sort (O(N log N)) for this task. The data distribution might affect real-world speed slightly, but the algorithmic advantage of Approach B holds in all cases.",
      "1": "They definitely don't take the same time! Sorting the entire list is O(N log N) — for 1 million items, that's roughly 20 million operations. Scanning once while tracking the top 10 is O(N) — about 1 million operations. The key insight is that sorting does unnecessary work: it carefully orders all 999,990 elements you don't care about.",
      "2": "Full sorting is overkill for this task. Sorting arranges ALL 1,000,000 grades in order (O(N log N)), but you only need the top 10. That's like alphabetising an entire library just to find 10 books. Approach B (scanning once) only does O(N) work and is the smarter choice when you need just a small number of top elements.",
      "3": "Correct! Scanning once is O(N), while full sorting is O(N log N) — a significant difference at scale. With 1 million items, sorting might take ~20 million operations versus ~1 million for a single scan. You can also use a min-heap of size 10, which keeps the tracking efficient. The key principle: when you only need the top k out of N items (where k is much smaller than N), avoid sorting the whole list."
    }
  },
  {
    "id": 88,
    "question": "You have a large list of sorted numbers. You want to find if the number '42' is in there. Is it faster to check every number one by one (Linear Search) or check the middle number and cut the list in half repeatedly (Binary Search)?",
    "options": [
      "They are the same",
      "Linear Search (more systematic)",
      "It depends",
      "Binary Search"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Binary Search is dramatically faster on sorted data. It works by checking the middle element and eliminating half the list each time, achieving O(log N) time complexity. For a list of 1 million numbers, Binary Search needs at most ~20 comparisons, while Linear Search might need up to 1 million. The prerequisite is that the list must be sorted — and since the question states it is, Binary Search is the clear winner.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "They are far from the same! Linear Search checks elements one by one — O(N) — while Binary Search halves the search space each step — O(log N). For a list of 1 million sorted numbers, Linear Search could take up to 1,000,000 comparisons, but Binary Search needs at most about 20. That's a massive difference, especially as the list grows.",
      "1": "While Linear Search is simple and systematic, simplicity doesn't mean speed. Linear Search checks every element sequentially (O(N)), which is very slow for large lists. Binary Search exploits the fact that the list is sorted: by checking the middle element, it eliminates half the remaining candidates each step, achieving O(log N). Being 'systematic' isn't an advantage when a smarter strategy exists.",
      "2": "For a sorted list (as stated in the question), Binary Search is consistently faster — this doesn't depend on other factors. Binary Search is O(log N) versus Linear Search's O(N). The only scenario where Linear Search might be preferred is if the data is unsorted, since Binary Search requires sorted data. But since the question specifies the list is sorted, Binary Search is definitively faster.",
      "3": "Correct! Binary Search leverages the sorted order to eliminate half the remaining candidates with each comparison, giving it O(log N) time complexity. For perspective: in a sorted list of 1 billion numbers, Binary Search finds any element in at most ~30 steps, while Linear Search might need up to 1 billion. This 'divide and conquer' strategy is one of the most important ideas in computer science."
    }
  },
  {
    "id": 89,
    "question": "You are matching 100 students to 100 project topics. Each student ranks their preferences. You want a stable match where no two people would rather be with each other than their assigned partners. This is known as:",
    "options": [
      "The Knapsack Problem",
      "The Stable Marriage/Matching Problem",
      "The Sorting Problem",
      "The Traveling Salesman Problem"
    ],
    "correctAnswerIndex": 1,
    "explanation": "This describes the Stable Marriage/Matching Problem, famously solved by the Gale-Shapley algorithm (which won its creators a Nobel Prize in Economics). The goal is to find a 'stable' matching where no unmatched pair would both prefer each other over their current assignments. It's used in real life for matching medical residents to hospitals, students to schools, and organ donors to recipients.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "The Knapsack Problem is about a different challenge: choosing items with different weights and values to maximise the total value you can fit into a container with a weight limit. For example, packing a backpack for a hike. It doesn't involve matching people based on preferences — it's about optimising selection under constraints.",
      "1": "Correct! The Stable Marriage/Matching Problem, solved by the Gale-Shapley algorithm, finds a stable assignment where no two unmatched participants would both prefer each other over their current match. This algorithm won a Nobel Prize in Economics and is used in real-world systems like medical residency matching (NRMP), school admissions, and organ donation matching.",
      "2": "The Sorting Problem is about arranging elements in a specific order (e.g., smallest to largest). While sorting might be a step within the matching process (like ranking preferences), the core challenge here is about finding a stable pairing between two groups — a fundamentally different problem that requires a specialised algorithm like Gale-Shapley.",
      "3": "The Traveling Salesman Problem (TSP) asks: 'What's the shortest route that visits every city exactly once and returns home?' It's about optimising a single path through locations, not about matching people to positions based on preferences. TSP is a famous NP-hard problem, meaning there's no known efficient solution for large inputs."
    }
  },
  {
    "id": 90,
    "question": "In Reinforcement Learning, an agent learns by trial and error. If it always sticks to what it knows works (Exploitation), what might it miss?",
    "options": [
      "It will miss all the possible solutions if it is not systematic and thorough",
      "It will learn too fast and miss solutions which are not obvious",
      "It might miss similar solutions with different weights if they are already exploited",
      "It might miss a new, even better strategy that it has not tried before"
    ],
    "correctAnswerIndex": 3,
    "explanation": "This is the classic Exploration vs. Exploitation trade-off in Reinforcement Learning. An agent that only exploits (repeats known good actions) may get stuck with a decent but suboptimal strategy, never discovering that an untried action could yield much higher rewards. Think of always eating at your favourite restaurant — it's good, but you might never discover an even better one down the street because you never tried it.",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "This answer is too vague. The issue with pure exploitation isn't about being 'systematic and thorough' — it's specifically about never trying new actions. An agent exploiting known strategies is actually being systematic about what it knows; the problem is that it never ventures beyond its current knowledge to discover potentially better options. The concept is specifically called the exploration-exploitation trade-off.",
      "1": "Pure exploitation doesn't cause 'learning too fast' — in fact, it causes the opposite problem. The agent stops learning new things because it keeps repeating the same actions it already knows work. It gets stuck in a comfort zone. The real risk is that it settles for a 'good enough' strategy and never discovers a superior one that exists but remains untried.",
      "2": "This answer is on a tangential track. The issue isn't about solutions with 'different weights' — it's about entirely new, untried strategies. When an agent only exploits, it repeats actions it has already found rewarding and never experiments with unfamiliar actions. The key risk is missing a potentially much better strategy simply because the agent never explored it.",
      "3": "Exactly right! This is the exploration-exploitation trade-off, a fundamental concept in Reinforcement Learning. Pure exploitation means the agent always picks the action it currently believes is best, but its beliefs are based on limited experience. By never exploring (trying new, uncertain actions), it might settle for a local optimum — like always going to the same restaurant and never discovering a better one nearby."
    }
  },
  {
    "id": 91,
    "question": "You want to compress a text file. You notice the letter 'e' appears very often, while 'z' is rare. A smart algorithm would:",
    "options": [
      "Delete all 'z's since this is uncommon (pruning)",
      "Use a long code for 'e'",
      "Use a short code for 'e' and a long code for 'z'",
      "Use the same length code for all letters"
    ],
    "correctAnswerIndex": 2,
    "explanation": "This is the idea behind Huffman Coding, one of the most elegant algorithms in computer science. By assigning shorter binary codes to frequently occurring characters and longer codes to rare ones, the total file size is minimised. Since 'e' appears thousands of times, saving even 1 bit per occurrence adds up to massive savings, while the longer code for rare 'z' barely affects the total size.",
    "topic": "Problem Solving",
    "subtopic": "Algorithm Design",
    "feedback_map": {
      "0": "Deleting characters would be lossy compression — you'd permanently lose information and couldn't reconstruct the original file. If 'z' appears in words like 'zero' or 'puzzle', removing it would corrupt the text. Smart compression (like Huffman Coding) is lossless: it reduces file size while preserving every single character, so you can perfectly reconstruct the original.",
      "1": "Using a long code for 'e' would actually make the file bigger, not smaller! Since 'e' is the most common letter, every extra bit in its code gets multiplied across thousands of occurrences. The smart strategy is the opposite: give the shortest codes to the most frequent characters to minimise the total number of bits used.",
      "2": "Correct! This is the principle behind Huffman Coding. Frequent characters (like 'e') get short binary codes, and rare characters (like 'z') get longer ones. Since 'e' appears far more often, saving even 1 bit per 'e' adds up enormously. The longer code for 'z' barely matters because it's used so rarely. This approach is optimal — no other variable-length prefix code can produce a smaller file.",
      "3": "Using the same code length for all letters (like standard ASCII, which uses 8 bits per character) ignores the frequency information entirely. This is wasteful because you're spending the same number of bits on 'e' (which appears constantly) and 'z' (which barely appears). Huffman Coding exploits these frequency differences to achieve much better compression."
    }
  },
  {
    "id": 92,
    "question": "You are finding the lowest point in a valley (Optimization). You take steps downhill. If your steps are too small, what happens?",
    "options": [
      "You might reverse and go uphill instead (opposite direction)",
      "It takes a very long time to reach the bottom",
      "You might jump over the valley (hit jackpot)",
      "You might reach the bottom instantly without training"
    ],
    "correctAnswerIndex": 1,
    "explanation": "In optimisation, step size corresponds to the learning rate. When the learning rate is too small, each update moves the model parameters by a tiny amount. You're still heading in the right direction (downhill), but progress is painfully slow — requiring thousands or millions of extra iterations to converge. It's like walking to school one centimetre at a time: you'll get there eventually, but it'll take forever.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "Small steps won't cause you to go uphill. The direction of each step is determined by the gradient (the slope), not the step size. A small learning rate means you move in the correct direction (downhill) but cover very little ground per step. Going uphill would require a bug in the algorithm or a sign error — not a small step size.",
      "1": "Correct! A very small learning rate means each step moves you only a tiny distance toward the minimum. You're heading the right way, but progress is extremely slow — potentially requiring millions of iterations to reach the bottom. In practice, this wastes computational time and energy. The opposite problem (too-large steps) causes overshooting. Finding the right learning rate is one of the key challenges in training machine learning models.",
      "2": "Jumping over the valley is actually the problem with steps that are too large, not too small! When the learning rate is too high, each step overshoots the minimum and the optimisation bounces back and forth or even diverges. With too-small steps, the opposite happens: you barely move and converge very slowly, but you don't overshoot.",
      "3": "Reaching the bottom instantly would require getting lucky with your starting point or having a perfect step size — neither of which relates to taking too-small steps. A tiny learning rate guarantees slow convergence, not instant convergence. In fact, it's the furthest thing from instant: you'd need far more iterations than with an appropriately sized learning rate."
    }
  },
  {
    "id": 93,
    "question": "You have a 'Black Box' optimization problem where you don't know the math formula (e.g., tuning a car engine). You try random settings and keep the best ones, then slightly tweak them. This is similar to:",
    "options": [
      "Calculus",
      "Evolutionary/Genetic Algorithms",
      "Gradient Descent",
      "Linear Algebra"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Genetic Algorithms are inspired by biological evolution. They maintain a 'population' of candidate solutions, evaluate their fitness, select the best ones, and create new candidates through mutation (small random changes) and crossover (combining parts of two good solutions). They're ideal for black-box problems because they don't require any knowledge of the underlying mathematical formula — they only need to evaluate how good each solution is.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "Calculus relies on having a known mathematical function so you can compute derivatives (rates of change). In a black-box problem, you don't have access to the formula — you can only try inputs and observe outputs. Without a formula, you can't take derivatives, making calculus-based methods inapplicable. Genetic Algorithms work without formulas by mimicking natural selection.",
      "1": "Correct! Genetic Algorithms mirror the process described: try various settings (population), keep the best (selection), and tweak them (mutation and crossover). They're inspired by biological evolution and are powerful precisely because they work on black-box problems — you don't need to know the formula, only whether one solution is better than another. They're used in real-world applications like circuit design, game AI, and engineering optimisation.",
      "2": "Gradient Descent requires computing the gradient (slope) of a function, which means you need to know the mathematical formula to calculate how the output changes with each input. In a black-box scenario, the formula is unknown — you can only try settings and see the result. Without a gradient to follow, Gradient Descent can't operate. Genetic Algorithms are designed for exactly this kind of situation.",
      "3": "Linear Algebra is a branch of mathematics dealing with vectors, matrices, and linear equations. While it's a foundational tool used throughout machine learning, it's not an optimisation method itself. The scenario described — trying random settings, keeping the best, and tweaking — is the hallmark of Evolutionary/Genetic Algorithms, which don't require any specific mathematical framework."
    }
  },
  {
    "id": 94,
    "question": "In 'Stochastic Gradient Descent' (SGD), instead of using ALL data to calculate the slope, we use a single random sample or small batch. Why?",
    "options": [
      "It guarantees finding the global maximum",
      "It is more precise because of randomization instead of extrapolation",
      "It uses less memory and is therefore more efficient, leading to solutions faster",
      "It is much faster to calculate per step and adds noise that can help escape local minimums"
    ],
    "correctAnswerIndex": 3,
    "explanation": "SGD has two key advantages. First, computing the gradient on a small batch is dramatically faster than on the entire dataset — if you have 10 million data points, each step with Batch Gradient Descent processes all 10 million, while SGD might use just 32 or 64. Second, the randomness ('stochasticity') introduces helpful noise: this noise can jolt the optimisation out of shallow local minima that Batch GD might get trapped in.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "SGD does not guarantee finding the global maximum (or minimum). In fact, no practical optimisation method guarantees finding the global optimum for complex, non-convex problems like neural networks. SGD is a heuristic — it typically finds a good solution, but not necessarily the absolute best one. Its advantage is speed and the ability to escape shallow local minima through noise.",
      "1": "SGD is actually less precise per step than Batch Gradient Descent, not more. Because it uses a random subset of data, each individual gradient estimate is noisier. However, this apparent weakness is also a strength: the noise helps the optimiser escape shallow local minima. SGD compensates for the per-step imprecision by taking many more (but much cheaper) steps.",
      "2": "While it's true that SGD uses less memory per step (since it processes a small batch instead of the entire dataset), this answer misses the more important advantage: the noise introduced by random sampling helps escape local minima. Also, 'leading to solutions faster' is true but for the wrong reason stated — it's faster primarily because each step's gradient computation is much cheaper, not just because of memory savings.",
      "3": "Correct! SGD's two main benefits are: (1) Each step is much cheaper to compute — processing 32 samples instead of 10 million makes each iteration thousands of times faster. (2) The randomness in gradient estimates creates noise that can bounce the optimiser out of shallow local minima, potentially finding better solutions than Batch GD. This speed-noise combination makes SGD the standard method for training deep learning models."
    }
  },
  {
    "id": 95,
    "question": "You are tuning a 'Hyperparameter' like Learning Rate. You try values: 0.1, 0.01, 0.001. This is called:",
    "options": [
      "Manual Tuning",
      "Random Search",
      "Bayesian Optimization",
      "Grid Search"
    ],
    "correctAnswerIndex": 3,
    "explanation": "Grid Search systematically tries every value in a pre-defined set. When you specify a list of values like [0.1, 0.01, 0.001] and test each one, you're doing a Grid Search. If you had multiple hyperparameters, Grid Search would try every combination — forming a 'grid' of possibilities. It's simple and thorough, but can become expensive when you have many hyperparameters with many possible values.",
    "topic": "Problem Solving",
    "subtopic": "Optimisation",
    "feedback_map": {
      "0": "Manual Tuning implies a human is adjusting values based on intuition and observing results interactively — like a scientist tweaking dials by hand. The scenario described is more structured: you define a specific set of values upfront and systematically evaluate each one. This systematic, pre-defined approach is Grid Search. Manual tuning is less structured and more ad-hoc.",
      "1": "Random Search picks values randomly from a range (e.g., a random number between 0.0001 and 1.0), not from a pre-defined list. The scenario describes trying specific, pre-chosen values [0.1, 0.01, 0.001], which is Grid Search. Interestingly, research has shown that Random Search can sometimes outperform Grid Search because it explores the hyperparameter space more diversely.",
      "2": "Bayesian Optimization is a smarter, adaptive approach: it uses the results of previous trials to intelligently choose the next value to try. For example, if 0.01 worked better than 0.1, it might try 0.005 next. The scenario described is not adaptive — it simply tests a fixed, pre-defined list of values, which is Grid Search. Bayesian Optimization is more efficient but more complex to implement.",
      "3": "Correct! Grid Search systematically evaluates every value in a pre-defined set. With one hyperparameter and values [0.1, 0.01, 0.001], you test all three. With two hyperparameters, you'd test every combination (a 'grid'). Grid Search is simple and guarantees you find the best value within your grid, but it scales poorly — 5 hyperparameters with 10 values each means 100,000 combinations to test!"
    }
  },
  {
    "id": 96,
    "question": "Regularization (like L2 Weight Decay) adds a penalty to the loss function if the model weights become too large. What is the goal of this?",
    "options": [
      "To make the training faster",
      "To force the model to be simpler and prevent overfitting",
      "To make the model bigger",
      "To increase the error"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Regularization discourages the model from relying on very large weight values, which often indicate the model is memorising the training data (overfitting) rather than learning general patterns. By penalising large weights, L2 regularization nudges the model toward simpler solutions that generalise better to new, unseen data. Think of it as a rule that says: 'Keep it simple unless complexity is truly necessary.'",
    "topic": "AI-ML-DL",
    "subtopic": "Classical ML",
    "feedback_map": {
      "0": "Regularization doesn't speed up training — in fact, it can slightly slow it down because the model has an additional constraint to satisfy. The optimizer must balance minimising the original loss AND keeping weights small. The goal is better generalisation (performance on new data), not faster training. Techniques like learning rate scheduling or batch normalisation are more directly related to training speed.",
      "1": "Correct! Large weights allow a model to create complex, wiggly decision boundaries that fit the training data perfectly but fail on new data — this is overfitting. L2 regularization adds a penalty proportional to the square of the weights, pushing them toward smaller values. This forces the model to find simpler patterns that capture the true underlying trends rather than memorising noise in the training data.",
      "2": "Regularization actually works in the opposite direction — it constrains the model to be simpler, not bigger. By penalising large weights, it effectively limits the model's capacity to create overly complex decision boundaries. A model with smaller weights behaves more smoothly and is less likely to overfit. If you wanted a bigger model, you'd add more layers or neurons, not regularization.",
      "3": "While regularization does add a penalty term to the loss function (which technically increases the total loss value), the goal is not to increase error. The purpose is to trade a small increase in training error for a much larger decrease in test error. By preventing the model from overfitting to training data, regularization improves the model's performance on new, unseen data — which is what actually matters."
    }
  },
  {
    "id": 97,
    "question": "You build a chatbot that answers questions by searching a database. If the database grows from 100 documents to 1,000,000 documents, your search gets very slow. What system design component do you need?",
    "options": [
      "A shorter prompt",
      "Larger screens to accommodate more data display",
      "A faster search algorithm to process the data more efficiently",
      "An Index to quickly find relevant documents"
    ],
    "correctAnswerIndex": 3,
    "explanation": "An Index is a data structure (like a hash map, inverted index, or B-tree) that enables fast lookups without scanning every document. Think of it like the index at the back of a textbook — instead of reading every page to find a topic, you look it up in the index and jump straight to the right page. Indexes can reduce search from O(N) (check every document) to O(1) or O(log N), making million-document searches nearly instant.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "A shorter prompt affects how the chatbot formulates queries, not how fast the database can be searched. Even with the shortest possible prompt, if the database has no index, the system still needs to scan through all 1,000,000 documents to find relevant ones. The bottleneck is in the search/retrieval step, which requires an index to solve.",
      "1": "Screen size is a display concern and has nothing to do with search performance. The problem is that finding relevant documents among 1,000,000 is computationally slow — this happens on the server/backend side, long before anything reaches a screen. System design focuses on backend infrastructure like indexing, caching, and efficient data structures.",
      "2": "While a faster algorithm helps, even the best search algorithm is limited if it has to examine every document (O(N)). The fundamental solution is an Index, which is a pre-built data structure that organizes documents so you can find relevant ones without scanning everything. It's like the difference between a faster reader and a textbook index — the index fundamentally changes the approach rather than just speeding up the existing one.",
      "3": "Correct! An Index (such as an inverted index or hash map) pre-organises the data so lookups are nearly instant — O(1) or O(log N) instead of O(N). For example, an inverted index maps each word to the list of documents containing it, so searching for 'climate change' immediately returns matching documents without scanning all 1,000,000. This is exactly how search engines like Google work at scale."
    }
  },
  {
    "id": 98,
    "question": "You are designing a self-driving car system. The camera detects a pedestrian. The system must decide to brake within 100 milliseconds. This is a requirement for:",
    "options": [
      "Cloud Computing",
      "Low Latency",
      "High Throughput",
      "Big Data"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Latency is the time delay between an input (detecting a pedestrian) and the corresponding output (deciding to brake). In safety-critical systems like self-driving cars, this delay must be extremely small — measured in milliseconds. A 100ms latency requirement means the entire pipeline (detection, decision, and brake signal) must complete in one-tenth of a second.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Cloud Computing involves processing data on remote servers over the internet. Sending data to the cloud and waiting for a response introduces significant network latency (often 50-200+ ms), making it unsuitable for a 100ms safety-critical decision. Self-driving car systems typically use on-board (edge) computing to keep latency as low as possible. Cloud computing is better suited for tasks where slight delays are acceptable.",
      "1": "Correct! Latency is the time delay between receiving input and producing output. A 100ms braking requirement is a strict low-latency constraint — the system must detect, decide, and act within one-tenth of a second. In self-driving cars, this requires powerful on-board processors (edge computing) rather than cloud-based processing, which would add network delays. Low latency is critical in any real-time safety system.",
      "2": "High Throughput refers to the volume of data or requests a system can handle per unit of time — for example, processing 1,000 images per second. The scenario is about response time (how quickly one braking decision is made), not about handling many requests simultaneously. A system can have high throughput but still have high latency, or vice versa. The 100ms constraint is specifically a latency requirement.",
      "3": "Big Data refers to extremely large datasets that require specialised tools to store and analyse (think petabytes of data). While self-driving cars do generate large amounts of sensor data, the 100ms braking requirement is about speed of response, not data volume. Big Data challenges are typically addressed during training and offline analysis, not during the split-second decision-making described here."
    }
  },
  {
    "id": 99,
    "question": "To fix the bottleneck above, you add more servers and a 'Load Balancer'. What does the Load Balancer do?",
    "options": [
      "It distributes the incoming user requests evenly across the servers",
      "It generates the answers faster and more efficiently",
      "It cools down the servers so that processing need not slow down",
      "It weighs the servers and allocates tasks proportionately"
    ],
    "correctAnswerIndex": 0,
    "explanation": "A Load Balancer is a system component that sits between users and a group of servers. Its job is to distribute incoming requests across the available servers so that no single server gets overwhelmed. Think of it like a traffic officer at a busy intersection — directing cars evenly across multiple lanes to prevent any one lane from getting gridlocked.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Correct! A Load Balancer receives all incoming requests and distributes them across multiple servers to prevent any single server from being overwhelmed. It can use various strategies: round-robin (taking turns), least-connections (send to the least busy server), or weighted distribution. This is a core concept in scalable system design — services like Google, Netflix, and Amazon use load balancers to handle millions of simultaneous users.",
      "1": "A Load Balancer doesn't generate answers or do any computation on the requests itself — it's purely a traffic director. It decides which server should handle each request, then forwards the request there. The actual processing (generating answers) is done by the backend servers. Think of it like a receptionist directing patients to available doctors — the receptionist doesn't treat anyone.",
      "2": "This describes physical cooling infrastructure (like fans or liquid cooling systems), not a Load Balancer. While server temperature management is important in data centres, a Load Balancer is a software/network component that routes requests across servers. It prevents overload through smart traffic distribution, not by managing physical temperature.",
      "3": "This option is a clever distractor! While some load balancers do use 'weighted' strategies (sending more traffic to more powerful servers), the word 'weighs' here is used literally (measuring physical weight), which is not what happens. A Load Balancer distributes requests based on factors like server availability, current load, or predefined rules — it doesn't physically weigh anything."
    }
  },
  {
    "id": 100,
    "question": "You design a recommendation system (like YouTube). It has two stages: Stage 1 picks 100 potential videos from millions. Stage 2 carefully ranks those 100. Why not just use Stage 2 on all millions of videos?",
    "options": [
      "Stage 2 is too slow or expensive to run on everything",
      "Stage 1 is harder to build",
      "Stage 1 is more accurate",
      "Stage 2 is heuristic in nature and may not arrive at the global minimum"
    ],
    "correctAnswerIndex": 0,
    "explanation": "This is the Retrieval-Ranking architecture used by most large-scale recommendation systems. Stage 2 (Ranking) uses a complex, computationally heavy model that deeply analyses each candidate — but running it on millions of videos would take too long and cost too much. Stage 1 (Retrieval) uses a fast, lightweight model to quickly narrow millions of options down to a manageable shortlist. It's like a job hiring process: a quick resume screen (Stage 1) narrows 10,000 applicants to 50, then detailed interviews (Stage 2) select the final hires.",
    "topic": "Problem Solving",
    "subtopic": "System Design",
    "feedback_map": {
      "0": "Correct! Stage 2 (Ranking) uses a sophisticated model that considers many features — user history, video quality, engagement predictions, and more. This deep analysis is computationally expensive. Running it on millions of videos per user per request would be impossibly slow and costly. Stage 1 (Retrieval) uses a fast, approximate method (like vector similarity) to narrow candidates to ~100, making Stage 2 feasible. This two-stage architecture is standard at YouTube, Netflix, TikTok, and other recommendation platforms.",
      "1": "Stage 1 (Retrieval) is typically simpler and easier to build than Stage 2. It uses lightweight techniques like vector similarity or collaborative filtering to quickly narrow the candidate pool. Stage 2 (Ranking) is the complex one — it uses deep neural networks with many input features. The reason for the two-stage design is computational cost, not build difficulty.",
      "2": "Stage 1 is actually less accurate than Stage 2 — that's by design. Stage 1 prioritises speed over precision, using approximate methods to quickly identify a reasonable set of candidates. Stage 2 then applies a more accurate but computationally expensive model to carefully rank those candidates. The two-stage system trades some recall in Stage 1 for the ability to apply high-accuracy ranking in Stage 2.",
      "3": "Stage 2 isn't heuristic — it's typically a sophisticated machine learning model that makes precise, learned predictions. The reason for not running it on everything is purely practical: it's too computationally expensive. The concept of 'arriving at a global minimum' relates to optimisation during model training, not to how a recommendation system serves predictions to users."
    }
  }
]